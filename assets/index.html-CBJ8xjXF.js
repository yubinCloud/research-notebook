import{_ as m,r,o,c as p,a as s,b as a,d as e,w as t,e as i}from"./app-s5JIphqh.js";const c={},h={href:"https://aclanthology.org/2022.emnlp-main.249",target:"_blank",rel:"noopener noreferrer"},g=s("p",null,"⭐⭐⭐⭐",-1),u=s("p",null,"EMNLP 2022, arXiv:2204.07496",-1),d={href:"https://github.com/DevSinghSachan/unsupervised-passage-reranking",target:"_blank",rel:"noopener noreferrer"},k=s("hr",null,null,-1),_={href:"https://aclanthology.org/2023.findings-emnlp.590",target:"_blank",rel:"noopener noreferrer"},y=s("p",null,"⭐⭐⭐⭐",-1),M=s("p",null,"EMNLP 2023, arXiv:2310.13243",-1),x={href:"https://github.com/ielab/llm-qlm",target:"_blank",rel:"noopener noreferrer"},L=s("hr",null,null,-1),v=s("h2",{id:"一、upr-论文速读",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#一、upr-论文速读"},[s("span",null,"一、UPR 论文速读")])],-1),w=s("blockquote",null,[s("p",null,"关于 Improving Passage Retrieval with Zero-Shot Question Generation 这篇论文")],-1),z=s("p",null,[a("论文提出了一个基于 LLM 的 re-ranker："),s("mark",null,"UPR"),a("（"),s("em",null,"Unsupervised Passage Re-ranker"),a("），它不需要任何标注数据用于训练，只需要一个通用的 PLM（pretrained LM），并且可以用在多种类型的检索思路上。")],-1),b=s("p",null,"给定一个 corpus 包含所有的 evidence documents，给定一个 question，由 Retriever 来从 corpus 中检索出 top-K passages，re-ranker 的任务就是把这 K 个 passages 做重新排序，期待重排后再交给 LLM 做 RAG 能提升效果。",-1),f=s("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240514214210.png",alt:"20240514214210",style:{zoom:"75%"}},null,-1),q=s("p",null,[a("本论文的工作中，使用 LLM 来为每一个 passage 计算一个 "),s("strong",null,"relevance score"),a("，然后按照 relevance scores 来对这些 passages 做排序。passages "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"z"),s("mi",null,"i")])]),s("annotation",{encoding:"application/x-tex"},"z_i")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 的 relevance score 的计算方式是：以 passage "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"z"),s("mi",null,"i")])]),s("annotation",{encoding:"application/x-tex"},"z_i")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 为条件，计算 LLM 生成 question "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"q")]),s("annotation",{encoding:"application/x-tex"},"q")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q")])])]),a(" 的 log-likelihood "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"log"),s("mo",null,"⁡"),s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",{mathvariant:"normal"},"∣"),s("msub",null,[s("mi",null,"z"),s("mi",null,"i")]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"\\log p(q|z_i)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mop"},[a("lo"),s("span",{style:{"margin-right":"0.01389em"}},"g")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])]),a("：")],-1),Q=s("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240514211839.png",alt:"20240514211839",style:{zoom:"75%"}},null,-1),P=s("p",null,[a("关于为什么使用 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"z"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(q|z)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mclose"},")")])])]),a(" 来计算 relevance score 而非用 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"z"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"q"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(z|q)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mclose"},")")])])]),a("，原因在于在假设 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"log"),s("mo",null,"⁡"),s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"z"),s("mi",null,"i")]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"\\log p(z_i)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mop"},[a("lo"),s("span",{style:{"margin-right":"0.01389em"}},"g")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])]),a(" 是都一样的话，按照 Bayes 公式来算的话，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"z"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(q|z)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mclose"},")")])])]),a(" 与 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"z"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"q"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(z|q)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mclose"},")")])])]),a(" 呈正相关的关系。此外，使用 p(q|z) 允许模型利用交叉注意力机制（cross-attention）在问题和段落之间建立联系。而且实验发现使用 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"z"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(q|z)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mclose"},")")])])]),a(" 效果更好。")],-1),R=s("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240514212620.png",alt:"20240514212620",style:{zoom:"75%"}},null,-1),S=s("p",null,[a("其实从感性上想一想，也是通过 prompt 让 LLM 去计算 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"z"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p(q|z)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"mclose"},")")])])]),a(" 来建模 question 和 passage 更合理。")],-1),G=i('<h2 id="二、开源-llm-本身就是强-zero-shot-的-qlm-re-ranker" tabindex="-1"><a class="header-anchor" href="#二、开源-llm-本身就是强-zero-shot-的-qlm-re-ranker"><span>二、开源 LLM 本身就是强 zero-shot 的 QLM re-ranker</span></a></h2><p><strong>QLM</strong>（<em>Query Likelihood Model</em>） 是指，通过计算特定 question 下 document 的概率来理解 docs 和 queries 的语义关系。QLM re-ranker 就是借助这个概率得出相关性分数从而做出排名，进而实现 re-rank。前面介绍的 UPR 就是一种 QLM re-ranker。</p><p>在前面介绍的 UPR 中，使用了 T0 LLM 模型作为 QLM 从而实现了有效的 re-rank，但是由于 T0 在许多 QG(Question Generation) 数据集上做了微调，所以该工作不能完全反映通用的 zero-shot 的 QLM ranking 场景。</p><p><strong>本工作研究了使用 LLaMA 和 Falcon 这两个 decoder-only 的模型作为 QLM 来做 re-rank 任务的表现</strong>，这两个 LLM 都没有在 QG 数据集上做训练。</p><h3 id="_2-1-多种-qlm-re-ranker" tabindex="-1"><a class="header-anchor" href="#_2-1-多种-qlm-re-ranker"><span>2.1 多种 QLM re-ranker</span></a></h3><p>本文工作设计了多种 QLM re-ranker，下面分别做一个介绍。</p><h4 id="_1-zero-shot-qlm-re-ranker" tabindex="-1"><a class="header-anchor" href="#_1-zero-shot-qlm-re-ranker"><span>1）Zero-shot QLM re-ranker</span></a></h4><p>类似于前面 UPR 的做法，借助于 QLM 计算出一个 relevance score，计算方法也一样（以 retrieved doc 为条件的 question 的概率）：</p>',8),N=s("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240514214509.png",alt:"20240514214509",style:{zoom:"75%"}},null,-1),U=s("h4",{id:"_2-bm25-插值的-re-ranker",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#_2-bm25-插值的-re-ranker"},[s("span",null,"2）BM25 插值的 re-ranker")])],-1),B=s("p",null,[a("除了使用 QLM 计算出来的分数 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"S"),s("mrow",null,[s("mi",null,"Q"),s("mi",null,"L"),s("mi",null,"M")])])]),s("annotation",{encoding:"application/x-tex"},"S_{QLM}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3283em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"Q"),s("span",{class:"mord mathnormal mtight"},"L"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"M")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a("，还融入第一阶段的检索器 BM25 给出的相关性分数，两者通过权重共同计算最终的 relevance score：")],-1),E=s("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240514214721.png",alt:"20240514214721",style:{zoom:"75%"}},null,-1),A=i('<h4 id="_3-few-shot-qlm-re-ranker" tabindex="-1"><a class="header-anchor" href="#_3-few-shot-qlm-re-ranker"><span>3）Few-shot QLM re-ranker</span></a></h4><p>在前面 zero-shot 的基础上，使用 LLM 时，设计一个 prompt template 并加入一些 few-shot exemplars。</p><h3 id="_2-2-实验" tabindex="-1"><a class="header-anchor" href="#_2-2-实验"><span>2.2 实验</span></a></h3><p>论文详细介绍了多个实验，感兴趣可以参考原论文，这里列出几个结论：</p><ul><li>在 QG 数据集（NS NARCO 数据集）上微调的 retriever 和 re-ranker 在所有数据集上表现都由于 zero-shot 的 retriever 和 QLM re-ranker，这是意料之中的，因为这些方法会受益于大量人工判断的 QA 训练数据，其知识可以有效地迁移到测试数据集中。</li><li>zero-shot 的 QLM 和经过 QG 指令微调的 QLM 表现出相似的竞争力，这一发现时令人惊讶的，这说明 pretrained-only 的 LLM 就具有强大的 zero-shot QLM 排名的能力。</li><li>如果 QG 任务没有出现在指令微调的数据中，那么指令微调反而会阻碍 LLM 的 QLM re-rank 能力。猜测原因在于，指令微调的模型往往更关注任务指令，而较少关注输入内容本身，但是评估 Query Likelihood 的最重要信息都在文档内容中，所以指令调优不利用 LLM 的 Query Likelihood 的估计。</li><li>BM25 插值策略的改进究竟有没有用，取决于具体的 LLM 模型。</li></ul><h3 id="_2-3-一个有效的-ranking-pipeline" tabindex="-1"><a class="header-anchor" href="#_2-3-一个有效的-ranking-pipeline"><span>2.3 一个有效的 ranking pipeline</span></a></h3><p>这篇论文工作（原文 4.3 节）还提出了一个有效的 ranking pipeline。</p><p>在第一阶段的 retriever 中，将 BM25 和 HyDE 结合作为 zero-shot first-stage hybird retriever，然后再使用 QLM 做 re-rank。</p><p>经过实验发现，这种方法可以与当前 SOTA 模型表现相当，重要的这种方法不需要任何训练。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>这两篇论文给了我们使用 LLM 来做 QLM re-rank 的思路，展现了通用的 LLM 本身具备强大的 QLM re-rank 的能力。</p>',11);function C(T,j){const n=r("ExternalLinkIcon"),l=r("center");return o(),p("div",null,[s("blockquote",null,[s("p",null,[a("论文："),s("a",h,[a("Improving Passage Retrieval with Zero-Shot Question Generation"),e(n)])]),g,u,s("p",null,[a("Code: "),s("a",d,[a("github.com/DevSinghSachan/unsupervised-passage-reranking"),e(n)])])]),k,s("blockquote",null,[s("p",null,[a("论文："),s("a",_,[a("Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking"),e(n)])]),y,M,s("p",null,[a("Code: "),s("a",x,[a("github.com/ielab/llm-qlm"),e(n)])])]),L,v,w,z,b,e(l,null,{default:t(()=>[f]),_:1}),q,e(l,null,{default:t(()=>[Q]),_:1}),s("blockquote",null,[P,e(l,null,{default:t(()=>[R]),_:1}),S]),G,e(l,null,{default:t(()=>[N]),_:1}),U,B,e(l,null,{default:t(()=>[E]),_:1}),A])}const D=m(c,[["render",C],["__file","index.html.vue"]]),O=JSON.parse(`{"path":"/arxiv/2204.07496/","title":"🐋 UPR：使用 LLM 来做检索后的 re-rank","lang":"zh-CN","frontmatter":{"title":"🐋 UPR：使用 LLM 来做检索后的 re-rank","permalink":"/arxiv/2204.07496/","author":"Bin Yu","createTime":"2024/05/14 21:00:00","head":[["script",{"id":"check-dark-mode"},";(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;if (um === 'dark' || (um !== 'light' && sm)) {document.documentElement.classList.add('dark');}})();"],["script",{"id":"check-mac-os"},"document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))"]]},"headers":[{"level":2,"title":"一、UPR 论文速读","slug":"一、upr-论文速读","link":"#一、upr-论文速读","children":[]},{"level":2,"title":"二、开源 LLM 本身就是强 zero-shot 的 QLM re-ranker","slug":"二、开源-llm-本身就是强-zero-shot-的-qlm-re-ranker","link":"#二、开源-llm-本身就是强-zero-shot-的-qlm-re-ranker","children":[{"level":3,"title":"2.1 多种 QLM re-ranker","slug":"_2-1-多种-qlm-re-ranker","link":"#_2-1-多种-qlm-re-ranker","children":[]},{"level":3,"title":"2.2 实验","slug":"_2-2-实验","link":"#_2-2-实验","children":[]},{"level":3,"title":"2.3 一个有效的 ranking pipeline","slug":"_2-3-一个有效的-ranking-pipeline","link":"#_2-3-一个有效的-ranking-pipeline","children":[]}]},{"level":2,"title":"总结","slug":"总结","link":"#总结","children":[]}],"readingTime":{"minutes":4.32,"words":1295},"git":{"updatedTime":1716026122000,"contributors":[{"name":"yubinCloud","email":"yubin_SkyWalker@yeah.net","commits":2}]},"filePathRelative":"notes/RAG/2204.URP：使用 LLM 来做检索后的 re-rank.md","categoryList":[{"type":10000,"name":"notes"},{"type":10003,"name":"RAG"}]}`);export{D as comp,O as data};
