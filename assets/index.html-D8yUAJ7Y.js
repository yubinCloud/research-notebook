import{_ as t,r as l,o,c as s,a as e,b as a,d as i,e as n}from"./app-s5JIphqh.js";const d={},u={href:"http://arxiv.org/abs/2312.10997",target:"_blank",rel:"noopener noreferrer"},p=e("p",null,"⭐⭐⭐⭐",-1),c=e("p",null,"同济，arXiv:2312.10997",-1),m=n('<h2 id="一、rag-的三种研究范式-naive、advanced、modular" tabindex="-1"><a class="header-anchor" href="#一、rag-的三种研究范式-naive、advanced、modular"><span>一、RAG 的三种研究范式：Naive、Advanced、Modular</span></a></h2><p>RAG 的研究范式持续进化，这里可以将其分成三个阶段：Naive RAG、Advanced RAG 和 Modular RAG。三种范式的对比如下图：</p><p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202418233.png" alt="image-20240510202418233"></p><h3 id="_1-1-naive-rag" tabindex="-1"><a class="header-anchor" href="#_1-1-naive-rag"><span>1.1 Naive RAG</span></a></h3><p>Naive RAG 是 <em>Retrieve-Read</em> 框架，包含了 indexing、retrieval 和 generation 三个过程：</p><ul><li>Indexing：对原始的 PDF、HTML、Word 或者 Markdown 等格式的数据进行清洗和抽取，转化为统一的 plain text 格式，并进而将其切分为更小的 chunks，之后，使用 embedding model 将这些 chunks 编码为 vector 并存入 vector db。这一步骤便于之后检索过程时做 similarity 计算。</li><li>Retrieval：对用户的 user query 使用 encoder 模型将其转化为 vector，然后计算 query vector 和 chunks vector 之间的相似度，从 corpus 中找到最相似的 K 个 chunks，这些检索得到的 chunks 被用于扩展之后 prompt 的上下文。</li><li>Generation：将 prompt template + user query + retrieved docs 输入给 LLM，让其完成响应生成。</li></ul><p>但是 Naive RAG 具有几个明显的缺点：</p><ul><li>retrieval 阶段可能会检索到不相关的信息，同时丢失一些重要信息</li><li>检索到的不相关的信息很可能会影响 LLM 的回复生成，降低生成的质量</li><li>当从多个 source 中检索到相似信息时，可能导致 LLM 出现重复响应的问题（即同样意思的话会反复说多次）</li><li>具有复杂推理的问题（如 multi-hop 问题），可能单次检索不足以获取足够的上下文信息</li><li>LLM 可能会过度依赖检索到的信息，导致 LLM 的输出只反映了检索到的内容，却没有添加深入思考或者合成的信息</li></ul><h3 id="_1-2-advanced-rag" tabindex="-1"><a class="header-anchor" href="#_1-2-advanced-rag"><span>1.2 Advanced RAG</span></a></h3>',9),h={href:"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6",target:"_blank",rel:"noopener noreferrer"},g=n('<p>Advanced RAG 以提高检索质量为重点来改进 Naive RAG。在 Naive RAG 的基础上，它引入了 pre-retrieval 和 post-retrieval 的阶段，在这两个阶段里，可以施加一些改进检索的技术。</p><ul><li><p><strong>Pre-retrieval process</strong>：在做检索之前加的一个阶段，主要关注的是<strong>优化索引和优化原始查询</strong>：</p><ul><li><p>优化索引的目标是提高被索引内容的质量。可以使用的策略包括：增强数据粒度、优化索引结构、添加元数据、对齐优化、混合检索等等</p></li><li><p>优化原始查询的目标是让 user query 的问题更加清晰，使之更加适合检索任务。可以使用的策略有：查询重写、查询转换、查询扩展等</p></li></ul></li><li><p><strong>Post-retrieval process</strong>：在检索到相关文档后，直接将所有相关文档输入给 LLM 会让它信息过载，不相关的信息会淡化 LLM 对关键细节的理解，甚至会对其产生误导。因此，这一阶段主要可以做的工作包括 re-rank 和 context compressing:</p><ul><li><p>re-rank：对检索到的文档重新排序，将最相关的内容放到前面</p></li><li><p>context compressing：压缩检索到的上下文，尽量只保留关键信息</p></li></ul></li></ul><h3 id="_1-3-modular-rag" tabindex="-1"><a class="header-anchor" href="#_1-3-modular-rag"><span>1.3 Modular RAG</span></a></h3><p>Modular RAG 通过引入多种策略、增加多个模块来提升 RAG 的各个相关组件。</p><ul><li>它允许根据具体的问题来调整模块和流程，例如引入搜索模块、记忆模块、额外生成模块等新模块，以扩展 RAG 的功能。</li><li>允许调整模块之间的顺序和连接方式，如对齐模块、添加或替换模块等，以适应不同任务和场景，与 LLM 的其他技术（如提示工程、知识蒸馏等）相结合，提高模型性能，并最终通过调整模块和流程，使 RAG 技术能够适应各种下游任务，提高通用性。</li></ul><p><strong>New Modules</strong> 的相关工作有：</p><ul><li>KnowledGPT：将检索数据源扩展到了 Knowledge Base</li><li>RAF-Fusion：引入 multi-query 策略，来扩展 user query 的视角，从而覆盖更加广阔的检索面</li><li>Selfmem：引入 memory module，memory 包含检索到的文档和 LLM 自己之前的生成，并使用 memory 来增强 generator</li><li>GenRead：将 retrieve-then-read 改为 generate-then-read 来提升效果，并提出进一步两者融合的思路</li><li>UPRISE：使用 prompt retriever 来检索 prompt 从而增强 LLM 的 zero-shot 能力</li></ul><p><strong>New Patterns</strong> 的相关工作有：</p><ul><li>Rewrite-Retrieve-Read：利用 LLM 的能力去修正 user query</li><li>GenRead：使用 Generate-Read 替换了 Retrieval-Read</li><li>Recite-Read：通过 prompt 让 LLM 先背诵再回答，从而将原先的知识密集型任务分解为两个步骤：knowledge-recitation 和 task-execution</li><li>HyDE：让 LLM 先根据 user query 生成一个杜撰的 doc，再使用杜撰的 doc 来通过 embedding 相似度来检索相关的 doc</li><li>Self-RAG：将 retrieval 改进为“自我反思检索”，让 LLM 每次生成一个文本段，生成前先反思一下是否需要检索，并利用相关机制从多个检索候选文档中选出最合适的文档</li></ul><h3 id="_1-4-rag-vs-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_1-4-rag-vs-fine-tuning"><span>1.4 RAG vs Fine-tuning</span></a></h3><p>用一个四象限图来表示如下，从 external knowledge 和 model adaption required 两个维度来阐释：</p><p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202714708.png" alt="image-20240510202714708"></p><p>对于 RAG 和 FT 的选择依赖于具体的场景。其实，RAG 和 FT 并没互相排斥的技术，而是而可以相互补充并在不同的层次上来提高 model 的能力。</p><h2 id="二、retrieval" tabindex="-1"><a class="header-anchor" href="#二、retrieval"><span>二、Retrieval</span></a></h2><p>RAG 依赖 retrieval 来从 data source 中获取相关文档。retrieval 需要考虑的问题有：检索源、检索粒度、检索的预处理、嵌入模型的选择等等。</p><h3 id="_2-1-retrieval-的数据结构" tabindex="-1"><a class="header-anchor" href="#_2-1-retrieval-的数据结构"><span>2.1 Retrieval 的数据结构</span></a></h3><ul><li><mark><strong>Unstructured Data</strong></mark>：比如文本。文本是使用最广泛的检索源，包括 Wikipedia Dump、domain-specific 数据。开放域 QA 主要使用的是 Wikipedia Dump，包括 HotpotQA、DPR 等。</li><li><mark><strong>Semi-structured Data</strong></mark>：通常指包含文本和表格信息的组合的数据，例如 PDF。处理半结构化数据对于 RAG 来说是很有挑战的，因为在融合文本和表结构存在很多困难，可以参考相关领域研究。</li><li><mark><strong>Structured Data</strong></mark>：比如 Knowledge Graph，这些信息经过了验证，可以提供更加精确的信息，比如 KnowledGPT 就是利用 KB 来增强 RAG 模型。当然另一方面，这种结构化数据需要额外的努力去构建、验证和维护结构化的数据库。</li><li><mark><strong>LLMs-Generated Content</strong></mark>：RAG 强调引入外部辅助知识，但其实利用好 LLM 内部的参数化知识也很重要。像 SKR 论文就将问题划分为已知和未知，已知问题直接交由 LLM 依靠内部知识来解决。像 Selfmem 论文就是也将 LLM 生成的信息作为增强上下文。</li></ul><h3 id="_2-2-retrieval-粒度" tabindex="-1"><a class="header-anchor" href="#_2-2-retrieval-粒度"><span>2.2 Retrieval 粒度</span></a></h3><p>检索粒度的选择会明显影响模型的效果，因为 LLM 会受到无关上下文的影响并因此导致生成质量降低。</p><ul><li>粗粒度的检索单元可以为 question 提供更多相关信息，但也会包含较多的冗余内容从而分散 retriever 和 LLM 在下游任务的注意力。</li><li>细粒度的检索单元让相关信息的密度更大，但会增加检索的负担，并且不能保证语义的完整性和所需知识的满足。</li></ul><p>在文本数据中，检索粒度从细到粗可以大致分为：token、phrase、sentence、proposition、chunk、document。</p><p>在 Knowledge Graph 中，检索粒度包括：entity、triplet、sub-graph。</p><blockquote><p>proposition 的检索粒度可以参考 Dense X 工作。</p></blockquote><h3 id="_2-3-索引优化-indexing-optimization" tabindex="-1"><a class="header-anchor" href="#_2-3-索引优化-indexing-optimization"><span>2.3 索引优化（Indexing Optimization）</span></a></h3><p>indexing 阶段的工作包括：将 documents 预处理、分割，并转换为 embedding 存入 vector database 中。</p><p>索引构建的质量决定了能否在检索阶段获得正确的上下文。</p><h4 id="_1-chunk-策略" tabindex="-1"><a class="header-anchor" href="#_1-chunk-策略"><span>1）chunk 策略</span></a></h4><p>最常见的方法就是按照固定数量的 tokens 来将 document 切分为 chunks，token 数量一般为 100、256、512。当然，这个过程存在很多优化，需要在语义完整性和无关信息冗余之间做出平衡：</p><ul><li>较大的 chunk 会包含较多的无关信息，影响 LLM 的生成质量，同时难以检索（因为无关信息会影响嵌入表示）</li><li>较小的 chunk 具有更高密度的相关信息，但是语义完整性较差，难以解决多条推理等复杂问题</li></ul><h4 id="_2-元数据附件" tabindex="-1"><a class="header-anchor" href="#_2-元数据附件"><span>2）元数据附件</span></a></h4><p>可以使用 metadata 来丰富 chunk 的信息，比如 filename、author、category 或者 timestamp，然后就可以基于元数据来做过滤从而限制检索范围。比如检索过程可以为 document 的 timestamp 分配不同的权重从而实现 time-aware 的 RAG，从而确保知识的新鲜性。</p><p>还可以人工构建元数据，比如添加段落的 summary，或者引入假设性 question（被称为反向 HyDE）。</p><h4 id="_3-结构索引" tabindex="-1"><a class="header-anchor" href="#_3-结构索引"><span>3）结构索引</span></a></h4><p>增强信息检索的一种有效方法就是为 document 建立层次结构，比如分层索引结构、知识图谱索引结构等。</p><h3 id="_2-4-查询优化-query-optimization" tabindex="-1"><a class="header-anchor" href="#_2-4-查询优化-query-optimization"><span>2.4 查询优化（Query Optimization）</span></a></h3><p>Naive RAG 的一个挑战是：它直接依赖于 user 的原始查询作为检索的基础，原始查询可能由于本身表达存在问题、与知识库存在 gap 等原因，导致检索效果不好。另外，LLM 在处理具有多种含义的专业词汇缩写时也会遇到困难。</p><p>所以，对 user query 进行优化，使之变为一个效果更好的查询是一个重要的改进方向。</p><h4 id="_1-query-expansion" tabindex="-1"><a class="header-anchor" href="#_1-query-expansion"><span>1）Query Expansion</span></a></h4><p>思路是为一个 single query 丰富其内容，提供更多的上下文，从而确保生成的答案的最佳相关性。具体实现方法包括：</p><ul><li><strong>Multi-Query</strong>：比如 <em>RAG-Fusion</em>，将一个 user query 扩展成多个 query 并行执行。</li><li><strong>Sub-Query</strong>：将一个 query 规划为多个 sub-query，这些 sub-query 的组合可以更加丰富地回答 original question。比如，*<em>least-to-most prompting</em> 方法将一个 complex question 分解为多个 simpler sub-questions 来解决。</li><li><strong>Chain-of-Verification</strong>：扩展后的 query 经过 LLM 验证可以表现出更好的可靠性。可以参考 <em>Chain-of-Verification</em> 论文。</li></ul><h4 id="_2-query-transformation" tabindex="-1"><a class="header-anchor" href="#_2-query-transformation"><span>2）Query Transformation</span></a></h4><p>核心思想是基于一个 transformed query 而不是 user original query 去检索 chunks。具体实现方法包括：</p><ul><li><strong>Query Rewrite</strong>：原始的查询在现实世界中并不一定是一个最好的检索，因此可以通过 prompt LLM 的方式去 rewrite queries。相关工作可以参考 EMNLP 2023 的 <em>Query Rewriting in Retrieval-Augmented Large Language Models</em>。</li><li>基于 prompt 工程让 LLM 生成基于原始 query 来生成一个 query：比如 <em>HyDE</em> 让 LLM 先生成一个杜撰文档用于 retrieval；<em>Step-Back Prompting</em> 先针对 user query 生成一个更高层次的 query 来检索相关事实用于辅助回答。</li></ul><h4 id="_3-query-routing" tabindex="-1"><a class="header-anchor" href="#_3-query-routing"><span>3）Query Routing</span></a></h4><p>思想是根据不同 query 路由到某个特定的合适的 RAG pipeline 中，从而可以构建出能够适用于不同场景的通用 RAG 系统。</p>',45),v=e("li",null,[e("strong",null,"Metadata Router / Filter"),a("：先从 query 中提取出关键字或实体，然后根据 chunk 中的关键字或 metadata 来过滤，从而缩小搜索范围")],-1),_=e("strong",null,"Semantic Router",-1),R={href:"https://github.com/aurelio-labs/semantic-router",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,"也可以将 metadata 方法与 semantic route 方法结合，来增强 query routing。",-1);function L(y,f){const r=l("ExternalLinkIcon");return o(),s("div",null,[e("blockquote",null,[e("p",null,[a("论文："),e("a",u,[a("Retrieval-Augmented Generation for Large Language Models: A Survey"),i(r)])]),p,c]),m,e("blockquote",null,[e("p",null,[a("可参考资料："),e("a",h,[a("Advanced RAG Techniques: an Illustrated Overview"),i(r)])])]),g,e("ul",null,[v,e("li",null,[_,a("：利用 query 的语义信息来路由。详细使用可以参考 "),e("a",R,[a("github.com/aurelio-labs/semantic-router"),i(r)])])]),k])}const q=t(d,[["render",L],["__file","index.html.vue"]]),G=JSON.parse(`{"path":"/arxiv/2312.10997/","title":"🐋 RAG for LLM 综述（23年12月）","lang":"zh-CN","frontmatter":{"title":"🐋 RAG for LLM 综述（23年12月）","permalink":"/arxiv/2312.10997/","author":"Bin Yu","createTime":"2024/05/01 12:00:00","head":[["script",{"id":"check-dark-mode"},";(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;if (um === 'dark' || (um !== 'light' && sm)) {document.documentElement.classList.add('dark');}})();"],["script",{"id":"check-mac-os"},"document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))"]]},"headers":[{"level":2,"title":"一、RAG  的三种研究范式：Naive、Advanced、Modular","slug":"一、rag-的三种研究范式-naive、advanced、modular","link":"#一、rag-的三种研究范式-naive、advanced、modular","children":[{"level":3,"title":"1.1 Naive RAG","slug":"_1-1-naive-rag","link":"#_1-1-naive-rag","children":[]},{"level":3,"title":"1.2 Advanced RAG","slug":"_1-2-advanced-rag","link":"#_1-2-advanced-rag","children":[]},{"level":3,"title":"1.3 Modular RAG","slug":"_1-3-modular-rag","link":"#_1-3-modular-rag","children":[]},{"level":3,"title":"1.4 RAG vs Fine-tuning","slug":"_1-4-rag-vs-fine-tuning","link":"#_1-4-rag-vs-fine-tuning","children":[]}]},{"level":2,"title":"二、Retrieval","slug":"二、retrieval","link":"#二、retrieval","children":[{"level":3,"title":"2.1 Retrieval 的数据结构","slug":"_2-1-retrieval-的数据结构","link":"#_2-1-retrieval-的数据结构","children":[]},{"level":3,"title":"2.2 Retrieval 粒度","slug":"_2-2-retrieval-粒度","link":"#_2-2-retrieval-粒度","children":[]},{"level":3,"title":"2.3 索引优化（Indexing Optimization）","slug":"_2-3-索引优化-indexing-optimization","link":"#_2-3-索引优化-indexing-optimization","children":[]},{"level":3,"title":"2.4 查询优化（Query Optimization）","slug":"_2-4-查询优化-query-optimization","link":"#_2-4-查询优化-query-optimization","children":[]}]}],"readingTime":{"minutes":9.49,"words":2847},"git":{"updatedTime":1715426070000,"contributors":[{"name":"yubinCloud","email":"yubin_SkyWalker@yeah.net","commits":2}]},"filePathRelative":"notes/RAG/2312.RAG 综述.md","categoryList":[{"type":10000,"name":"notes"},{"type":10003,"name":"RAG"}]}`);export{q as comp,G as data};
