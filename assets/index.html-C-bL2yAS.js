import{_ as s,r as a,o as n,c as i,a as e,b as t,d as r,e as l}from"./app-s5JIphqh.js";const p={},m={href:"https://openreview.net/forum?id=WZH7099tgfM",target:"_blank",rel:"noopener noreferrer"},c=e("p",null,"⭐⭐⭐",-1),d=e("p",null,"Google Research, ICLR 2023",-1),u=l('<h2 id="论文速读" tabindex="-1"><a class="header-anchor" href="#论文速读"><span>论文速读</span></a></h2><p>Chain-of-Thought（CoT） prompting 的方法通过结合 few-show prompt 的思路，让 LLM 能够挑战更具复杂性的问题。但是 CoT 的方法存在一个关键限制：<strong>它在需要泛化性来解决比 demonstration examples 更困难的问题的 task 上，通常表现不佳</strong>。</p><p>为了克服这个缺点，本论文提出了 <mark>least-to-most prompting</mark> 的方法，它先让 LLM 将原来的问题分解为多个需要预先解决的 sub-questions，然后依次按顺序让 LLM 去解决这些 sub-questions，在解决每个 sub-question 的时候，LLM 可以看到之前的每个 sub-question 以及回复。如下图：</p><p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/a434d2c2-aa4d-4a9a-8f95-2e1a330071fc.png" alt=""></p><p>可以看到，它包含两个 stage，每个 stage 都是通过 few-shot prompt 来实现的，并且整个过程没有任何 model 被训练：</p><ol><li><strong>Decomposition</strong>：这个阶段的 prompt 包含固定的几个用于演示 decomposition 的 few-shot exemplars，然后跟着需要被 decomposed 的 question</li><li><strong>Subproblem solving</strong>：这个阶段的 prompt 包含三个部分： <ul><li>固定的几个用于演示 subproblem 如何被解决的 few-shot exemplars</li><li>先前已经被 LLM 回答了的 subquestions 以及对应的生成的回答</li><li>接下来需要被回答的 question</li></ul></li></ol><p>最终，原先的 user question 作为最后一个 subquestion 被 LLM 解决。</p><h2 id="实验" tabindex="-1"><a class="header-anchor" href="#实验"><span>实验</span></a></h2><p>论文做了 symbolic manipulation、compositional generalization 和 math reasoning tasks 三个实验，并主要与 CoT 进行了对比。</p><p>总的来说，本文提出的 Least-to-Most Prompting 相比于 CoT 的优势主要在于：</p><ul><li><strong>在长度泛化方面更好</strong>。面对比 few-shot exemplars 更长的问题，比 CoT 解决地更好</li><li><strong>在困难泛化方面更好</strong>。面对比 few-shot exemplars 更困难的问题，也比 CoT 解决地更好</li></ul><h2 id="总结与分析" tabindex="-1"><a class="header-anchor" href="#总结与分析"><span>总结与分析</span></a></h2><p>论文指出，该方法的 decomposition prompt 不能很好地跨域泛化，在一个 domain 上 decomposition 的示例无法有效地用在另一个 domain（task）上。</p><p>总的来说，本工作提出了 least-to-most prompting 的方法，通过自顶向下的问题分解和自底向上的子问题解决实现了最终的解决问题。在该方法中，prompt 由以前的单向与 LLM 交流变成了双向的互动。通过双向交互来指导 LLM 仍然值得探索。</p>',14);function h(g,L){const o=a("ExternalLinkIcon");return n(),i("div",null,[e("blockquote",null,[e("p",null,[t("论文："),e("a",m,[t("Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"),r(o)])]),c,d]),u])}const k=s(p,[["render",h],["__file","index.html.vue"]]),_=JSON.parse(`{"path":"/paper/least-to-most-prompting/","title":"🐋 Least-to-Most Prompting 让 LLM 实现复杂推理","lang":"zh-CN","frontmatter":{"title":"🐋 Least-to-Most Prompting 让 LLM 实现复杂推理","permalink":"/paper/least-to-most-prompting/","author":"Bin Yu","createTime":"2024/05/10 10:44:05","head":[["script",{"id":"check-dark-mode"},";(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;if (um === 'dark' || (um !== 'light' && sm)) {document.documentElement.classList.add('dark');}})();"],["script",{"id":"check-mac-os"},"document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))"]]},"headers":[{"level":2,"title":"论文速读","slug":"论文速读","link":"#论文速读","children":[]},{"level":2,"title":"实验","slug":"实验","link":"#实验","children":[]},{"level":2,"title":"总结与分析","slug":"总结与分析","link":"#总结与分析","children":[]}],"readingTime":{"minutes":2.08,"words":624},"git":{"updatedTime":1716026122000,"contributors":[{"name":"yubinCloud","email":"yubin_SkyWalker@yeah.net","commits":4}]},"filePathRelative":"notes/LM/2205.Least-to-Most Prompting 让 LLM 实现复杂推理.md","categoryList":[{"type":10000,"name":"notes"},{"type":10002,"name":"LM"}]}`);export{k as comp,_ as data};
