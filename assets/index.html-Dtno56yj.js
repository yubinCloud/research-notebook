import{_ as d,r,o as c,c as p,a as e,b as t,d as n,w as s,e as a}from"./app-s5JIphqh.js";const g={},h={href:"http://arxiv.org/abs/2401.06954",target:"_blank",rel:"noopener noreferrer"},m=e("p",null,"⭐⭐⭐",-1),u=e("p",null,"Google Research, arXiv:2401.06954",-1),L=a('<h2 id="论文速读" tabindex="-1"><a class="header-anchor" href="#论文速读"><span>论文速读</span></a></h2><p>LLM 与 Retriever 之间存在一个 <mark>preference gap</mark>：大多数 retriever 被设计为 human-friendly，但是 LLM 的偏好与人类的却不一致：</p><ul><li><strong>ranking 方面</strong>：由于 LLM 的 self-attention 机制，模型可以集中任何 token 而无视其 position。但人类对于 position 还是很关注的。</li><li><strong>selection 方面</strong>：人类可以轻易地忽视掉与上下文无关的信息，但 LLM 却对于无关内容特别敏感。</li><li><strong>repetition 方面</strong>：人类往往不关心重复内容，甚至不喜欢重复内容，但是 repetition 却在对于 LLM 在衡量相关性的权重时很有帮助。</li></ul><p>论文原文设计了一些实验来证明 preference gap 确实存在，具体可以参考原论文。</p><p>为了弥补 LLM 和 Retriever 之间的 preference gap，过去的研究工作往往是集中于对 LLM 或 Retriever 进行微调，但<strong>其实无论是 LLM 还是 Retriever 都很可能是无法微调的</strong>。</p><blockquote><p>比如对于生产级的 Retriever，如 Google 或 Bing，都是不能被微调的。</p></blockquote>',6),_=e("mark",null,"BGM",-1),f=e("strong",null,"B",-1),v=e("strong",null,"G",-1),k=e("strong",null,"M",-1),M=e("img",{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240513220814.png",alt:"20240513220814",style:{zoom:"75%"}},null,-1),b=a('<p>这里的 Bridge 模型是可以训练的，训练过程分成了两个阶段：监督学习（SL）阶段和强化学习（RL）阶段。</p><h2 id="bridge-模型的训练" tabindex="-1"><a class="header-anchor" href="#bridge-模型的训练"><span>Bridge 模型的训练</span></a></h2><h3 id="阶段-1-sl-阶段" tabindex="-1"><a class="header-anchor" href="#阶段-1-sl-阶段"><span>阶段 1：SL 阶段</span></a></h3><p>首先是 Supervised Learning（SL）阶段。</p><p>SL 学习往往需要 golden passage sequence 作为每个 query 的 label 从而实现监督学习，但是 golden passage sequence 是一种理想的情况，由于实际应用中不存在这样的真是标签，且对于一个 query，有太多有效的段落组合方式，从中选出最理想的答案在计算上是不可行的。</p><p>于是本文使用 sliver passage sequence（SPS）作为训练标签，也就是次优的标签。因此我们首先需要合成出用于 SL 的 SPS 数据。</p><p>SPS 数据的合成关键是用了<strong>贪心搜索</strong>的思想，通过迭代，最开始的段落序列是一个 empty sequence，之后逐步添加能够提升 downstream task 表现的最佳 passage，并将其加入到 SPS 中，直到无法进一步改善性能为止。</p><p>sliver passage sequence 用于监督学习阶段，作为训练目标，帮助桥接模型学习如何从检索到的段落中选择和排序，以生成对 downstream task 最有帮助的段落序列。</p><h3 id="阶段-2-rl-阶段" tabindex="-1"><a class="header-anchor" href="#阶段-2-rl-阶段"><span>阶段 2：RL 阶段</span></a></h3><p>实验发现，只使用 SL 来训练 Bridge model 是不够的，SL only 的模型最终的表现并不好，原因可能就是稀疏的 supervision 以及缺少在 downstream results 上的 end-to-end training。</p><p>为了解决这些问题，论文进一步对 SL 训练后的 Bridge Model 做进一步的强化学习，RL 可以让 Bridge model 学习到 optimal passage sequence 所需要的更加复杂的操作（比如 repetition）。</p><p>RL 的使用方法是：</p><ul><li>downstream task 的 performance 被用来设计 <strong>reward</strong>。比如 QA 任务中的 BLEU 分数。</li><li>bridge model 就是需要训练的 <strong>policy model</strong>。</li><li><strong>action space</strong> 定义了模型可以采取的所有可能动作，在这篇论文中，action space 可能包括选择哪些 passages、它们的顺序、以及是否需要重复某些 passage 等。</li><li><strong>Environment</strong> 就是由 Retriever、Bridge Model、LLM 组成的整体。</li></ul><p>训练阶段的优化算法可以是任何 off-the-shelf RL 算法，论文提到了使用 PPO 作为优化算法。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>本论文提出的问题是现实存在的：LLM 和 Retriever 往往都是 Frozen 的，都是无法微调的，但两者之间的 preference gap 又是明显存在的。本文提出引入一个 Bridge Model 来填补这个 gap，但是本文提出的训练方法有点太复杂了，也许有进一步简化的思路。</p>',16);function B(R,S){const i=r("ExternalLinkIcon"),o=r("font"),l=r("center");return c(),p("div",null,[e("blockquote",null,[e("p",null,[t("论文："),e("a",h,[t("Bridging the Preference Gap between Retrievers and LLMs"),n(i)])]),m,u]),L,e("p",null,[t("本论文提出了 "),_,t("（"),f,t("ridging the "),v,t("ap between retrievers and LL"),k,t("s）框架来解决这个问题："),n(o,{color:"blue"},{default:s(()=>[t("它在 LLM 和 Retriever 之间额外添加了一个 seq2seq 的 Bridge 模型，这个 Bridge 模型的输入是 retrieved passages，输出是 LLM-friendly passages")]),_:1}),t("。（如下图的最三个模型）")]),n(l,null,{default:s(()=>[M]),_:1}),b])}const y=d(g,[["render",B],["__file","index.html.vue"]]),q=JSON.parse(`{"path":"/arxiv/2401.06954/","title":"🐋 BGM：为 LLM 和 Retriever 的偏好 gap 搭建一个 Bridge","lang":"zh-CN","frontmatter":{"title":"🐋 BGM：为 LLM 和 Retriever 的偏好 gap 搭建一个 Bridge","permalink":"/arxiv/2401.06954/","author":"Bin Yu","createTime":"2024/05/13 21:49:00","head":[["script",{"id":"check-dark-mode"},";(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;if (um === 'dark' || (um !== 'light' && sm)) {document.documentElement.classList.add('dark');}})();"],["script",{"id":"check-mac-os"},"document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))"]]},"headers":[{"level":2,"title":"论文速读","slug":"论文速读","link":"#论文速读","children":[]},{"level":2,"title":"Bridge 模型的训练","slug":"bridge-模型的训练","link":"#bridge-模型的训练","children":[{"level":3,"title":"阶段 1：SL 阶段","slug":"阶段-1-sl-阶段","link":"#阶段-1-sl-阶段","children":[]},{"level":3,"title":"阶段 2：RL 阶段","slug":"阶段-2-rl-阶段","link":"#阶段-2-rl-阶段","children":[]}]},{"level":2,"title":"总结","slug":"总结","link":"#总结","children":[]}],"readingTime":{"minutes":3.53,"words":1058},"git":{"updatedTime":1716026122000,"contributors":[{"name":"yubinCloud","email":"yubin_SkyWalker@yeah.net","commits":3}]},"filePathRelative":"notes/RAG/2401.BGM：为 LLM 和 Retriever 的偏好 gap 搭建一个 Bridge.md","categoryList":[{"type":10000,"name":"notes"},{"type":10003,"name":"RAG"}]}`);export{y as comp,q as data};
