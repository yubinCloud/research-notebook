---
title: 🐋 Contriever：对比学习来无监督训练文本检索器
permalink: /arxiv/2112.09118/
author: Bin Yu
createTime: 2024/05/11 11:08:00
---

>论文：[Unsupervised Dense Information Retrieval with Contrastive Learning](http://arxiv.org/abs/2112.09118)
>
>⭐⭐⭐⭐⭐
>
>Facebook Research, arXiv:2112.09118
>
>Code：[github.com/facebookresearch/contriever](https://github.com/facebookresearch/contriever)

## 一、论文速读

本文使用对比学习的方法来对文本检索模型做无监督学习训练，从而实现在多个领域的泛化性。

**提出的 motivation**：在有大量数据集的 domain 上做监督训练得到的 dense retriever 具有强大的表现，但是当被应用到其他 domain 时，表现可能还不如 BM25 这类经典方法。于是想到，无监督学习是迁移学习的一种自然选择，本文的研究问题就是：**有没有可能在无监督的情况下训练一个 dense retriever，并与 BM25 的性能相匹配**。

本工作提出了 <mark>Contriever</mark> 模型，采用 bi-encoder 架构，query 和 doc 分别进行编码，其相关性得分由两者的 vector representation 的 dot product 计算得出。经验表明，query encoder 与 doc encoder 采用相同的 encoder 通常可以在零样本迁移或者少样本学习的背景下提高鲁棒性（原 paper 第 3 节提出），因此本工作使用了相同的 encoder，encoder 基于 BERT 来进行训练。

论文的重点创新是其训练思路，下面详细介绍。

## 二、模型的训练

### 2.1 对比学习（Contrastive Learning）

训练数据包含 positive pairs 和 negative pairs，对比学习采用 InfoNCE 损失，具体如下论文所示：

![1715417066478](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1715417066478.png)

### 2.2 构建 positive pairs

对比学习的一个关键因素是如何从单个 input 中构建 positive pairs，本文的方法如下：

- **反完形填空任务**（**Inverse Cloze Task**，**ICT**）：是一种训练 retriever 的数据增强方法，经常被用来生成用于训练的正样本对，具体来说，ICT 的步骤如下：
  1. 文本分段：将 document 分割成若干的 segments
  2. 随机采样：从每个 segment 中随机采样出一个 span 的文本作为 query，该 segment 的剩余部分作为 context，(query, context) 就可以作为一个正样本
  3. 训练检索器：训练 retriever，使其能够根据 query 检索出原始的 context
- **Independent cropping**：从一个 document 中，完全独立随机地采样出两个 span tokens 作为正样本对
- **Additional data augmentation**：额外的数据增强，如随即删除单词、替换或者屏蔽等。

在之后的消融实验中，**该工作发现使用“independent cropping”来训练 retriever 是 ICT 的一个有力替代方案**。

### 2.3 构建大量的 negative pairs

这里主要用了两种思路：in-batch negatives 方法和 MoCo 方法。

in-batch negatives 方法已经在其他论文讲解中介绍了。

<mark>Moco</mark>（Momentum Contrast）是一种用于无监督或自监督学习的对比学习方法，它在处理大规模数据集时特别有效，因为它可以高效地利用大量的负样本。**MoCo 的核心思想是使用一个动态更新的 queue 来存储 negative example 的 vector representation**。

MoCo 方法的几个关键步骤如下：

1. **正样本对**：对于每个 input，先按照前面的方法构造一个 positive pair
2. **负样本队列**：维护了一个 negative queue，用于存储之前 batch 的 negative example 的 vector representation，其大小是固定预先设定的。每个训练步骤中，最新的负样本表示会被加入到 queue 中，而队首则会被移除。queue 反映了最近的训练状态。
3. **Query Network**：该网络负责对 input query 生成 vector representation，训练过程中会通过梯度下降进行更新
4. **动量编码器**（momentum encoder）：MoCo 中，负样本的 representation 由动量编码器生成，该编码器的参数更新也不是通过梯度下降更新，而是通过“指数移动平均”来更新（具体可参考原论文）。这意味着动量编码器的参数更新速度较慢，从而在训练过程中提供了更加平滑和一致的负样本表示。
5. **对比损失**：使用对比损失来训练 encoder，对于每个正样本对，模型需要将其与队列中的负样本区分开来。

以上就是 MoCo 的思路。

## 三、结论

该工作主要探索了使用 MoCo 技术来基于对比学习和无监督学习来训练 retriever，并发现它表现出良好的检索性能，具有不错的泛化性。

如果继续对其微调的话，可以进一步改进其表现，从而产生强大的结果。
