---
title: 🐋 RAG for LLM 综述（23年12月）
permalink: /arxiv/2312.10997/
author: Bin Yu
createTime: 2024/05/01 12:00:00
---

> 论文：[Retrieval-Augmented Generation for Large Language Models: A Survey](http://arxiv.org/abs/2312.10997)
> 
> ⭐⭐⭐⭐
> 
> 同济，arXiv:2312.10997

## 一、RAG  的三种研究范式：Naive、Advanced、Modular

RAG 的研究范式持续进化，这里可以将其分成三个阶段：Naive RAG、Advanced RAG 和 Modular RAG。三种范式的对比如下图：

![image-20240510202418233](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202418233.png)

### 1.1 Naive RAG

Naive RAG 是 *Retrieve-Read* 框架，包含了 indexing、retrieval 和 generation 三个过程：

- Indexing：对原始的 PDF、HTML、Word 或者 Markdown 等格式的数据进行清洗和抽取，转化为统一的 plain text 格式，并进而将其切分为更小的 chunks，之后，使用 embedding model 将这些 chunks 编码为 vector 并存入 vector db。这一步骤便于之后检索过程时做 similarity 计算。
- Retrieval：对用户的 user query 使用 encoder 模型将其转化为 vector，然后计算 query vector 和 chunks vector 之间的相似度，从 corpus 中找到最相似的 K 个 chunks，这些检索得到的 chunks 被用于扩展之后 prompt 的上下文。
- Generation：将 prompt template + user query + retrieved docs 输入给 LLM，让其完成响应生成。



但是 Naive RAG 具有几个明显的缺点：

- retrieval 阶段可能会检索到不相关的信息，同时丢失一些重要信息
- 检索到的不相关的信息很可能会影响 LLM 的回复生成，降低生成的质量
- 当从多个 source 中检索到相似信息时，可能导致 LLM 出现重复响应的问题（即同样意思的话会反复说多次）
- 具有复杂推理的问题（如 multi-hop 问题），可能单次检索不足以获取足够的上下文信息
- LLM 可能会过度依赖检索到的信息，导致 LLM 的输出只反映了检索到的内容，却没有添加深入思考或者合成的信息

### 1.2 Advanced RAG

> 可参考资料：[Advanced RAG Techniques: an Illustrated Overview](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)

Advanced RAG 以提高检索质量为重点来改进 Naive RAG。在 Naive RAG 的基础上，它引入了 pre-retrieval 和 post-retrieval 的阶段，在这两个阶段里，可以施加一些改进检索的技术。

- **Pre-retrieval process**：在做检索之前加的一个阶段，主要关注的是**优化索引和优化原始查询**： 

  - 优化索引的目标是提高被索引内容的质量。可以使用的策略包括：增强数据粒度、优化索引结构、添加元数据、对齐优化、混合检索等等

  - 优化原始查询的目标是让 user query 的问题更加清晰，使之更加适合检索任务。可以使用的策略有：查询重写、查询转换、查询扩展等

- **Post-retrieval process**：在检索到相关文档后，直接将所有相关文档输入给 LLM 会让它信息过载，不相关的信息会淡化 LLM 对关键细节的理解，甚至会对其产生误导。因此，这一阶段主要可以做的工作包括 re-rank 和 context compressing: 

  - re-rank：对检索到的文档重新排序，将最相关的内容放到前面

  - context compressing：压缩检索到的上下文，尽量只保留关键信息

### 1.3 Modular RAG

Modular RAG 通过引入多种策略、增加多个模块来提升 RAG 的各个相关组件。

- 它允许根据具体的问题来调整模块和流程，例如引入搜索模块、记忆模块、额外生成模块等新模块，以扩展 RAG 的功能。
- 允许调整模块之间的顺序和连接方式，如对齐模块、添加或替换模块等，以适应不同任务和场景，与 LLM 的其他技术（如提示工程、知识蒸馏等）相结合，提高模型性能，并最终通过调整模块和流程，使 RAG 技术能够适应各种下游任务，提高通用性。

**New Modules** 的相关工作有：

- KnowledGPT：将检索数据源扩展到了 Knowledge Base
- RAF-Fusion：引入 multi-query 策略，来扩展 user query 的视角，从而覆盖更加广阔的检索面
- Selfmem：引入 memory module，memory 包含检索到的文档和 LLM 自己之前的生成，并使用 memory 来增强 generator
- GenRead：将 retrieve-then-read 改为 generate-then-read 来提升效果，并提出进一步两者融合的思路
- UPRISE：使用 prompt retriever 来检索 prompt 从而增强 LLM 的 zero-shot 能力

**New Patterns** 的相关工作有：

- Rewrite-Retrieve-Read：利用 LLM 的能力去修正 user query
- GenRead：使用 Generate-Read 替换了 Retrieval-Read
- Recite-Read：通过 prompt 让 LLM 先背诵再回答，从而将原先的知识密集型任务分解为两个步骤：knowledge-recitation 和 task-execution
- HyDE：让 LLM 先根据 user query 生成一个杜撰的 doc，再使用杜撰的 doc 来通过 embedding 相似度来检索相关的 doc
- Self-RAG：将 retrieval 改进为“自我反思检索”，让 LLM 每次生成一个文本段，生成前先反思一下是否需要检索，并利用相关机制从多个检索候选文档中选出最合适的文档

### 1.4 RAG vs Fine-tuning

用一个四象限图来表示如下，从 external knowledge 和 model adaption required 两个维度来阐释：

![image-20240510202714708](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202714708.png)

对于 RAG 和 FT 的选择依赖于具体的场景。其实，RAG 和 FT 并没互相排斥的技术，而是而可以相互补充并在不同的层次上来提高 model 的能力。

## 二、Retrieval

RAG 依赖 retrieval 来从 data source 中获取相关文档。retrieval 需要考虑的问题有：检索源、检索粒度、检索的预处理、嵌入模型的选择等等。

### 2.1 Retrieval 的数据结构

- <mark>**Unstructured Data**</mark>：比如文本。文本是使用最广泛的检索源，包括 Wikipedia Dump、domain-specific 数据。开放域 QA 主要使用的是 Wikipedia Dump，包括 HotpotQA、DPR 等。
- <mark>**Semi-structured Data**</mark>：通常指包含文本和表格信息的组合的数据，例如 PDF。处理半结构化数据对于 RAG 来说是很有挑战的，因为在融合文本和表结构存在很多困难，可以参考相关领域研究。
- <mark>**Structured Data**</mark>：比如 Knowledge Graph，这些信息经过了验证，可以提供更加精确的信息，比如 KnowledGPT 就是利用 KB 来增强 RAG 模型。当然另一方面，这种结构化数据需要额外的努力去构建、验证和维护结构化的数据库。
- <mark>**LLMs-Generated Content**</mark>：RAG 强调引入外部辅助知识，但其实利用好 LLM 内部的参数化知识也很重要。像 SKR 论文就将问题划分为已知和未知，已知问题直接交由 LLM 依靠内部知识来解决。像 Selfmem 论文就是也将 LLM 生成的信息作为增强上下文。

### 2.2 Retrieval 粒度

检索粒度的选择会明显影响模型的效果，因为 LLM 会受到无关上下文的影响并因此导致生成质量降低。

- 粗粒度的检索单元可以为 question 提供更多相关信息，但也会包含较多的冗余内容从而分散 retriever 和 LLM 在下游任务的注意力。
- 细粒度的检索单元让相关信息的密度更大，但会增加检索的负担，并且不能保证语义的完整性和所需知识的满足。

在文本数据中，检索粒度从细到粗可以大致分为：token、phrase、sentence、proposition、chunk、document。

在 Knowledge Graph 中，检索粒度包括：entity、triplet、sub-graph。

> proposition 的检索粒度可以参考 Dense X 工作。

### 2.3 索引优化（Indexing Optimization）

indexing 阶段的工作包括：将 documents 预处理、分割，并转换为 embedding 存入 vector database 中。

索引构建的质量决定了能否在检索阶段获得正确的上下文。

#### 1）chunk 策略

最常见的方法就是按照固定数量的 tokens 来将 document 切分为 chunks，token 数量一般为 100、256、512。当然，这个过程存在很多优化，需要在语义完整性和无关信息冗余之间做出平衡：

- 较大的 chunk 会包含较多的无关信息，影响 LLM 的生成质量，同时难以检索（因为无关信息会影响嵌入表示）
- 较小的 chunk 具有更高密度的相关信息，但是语义完整性较差，难以解决多条推理等复杂问题

#### 2）元数据附件

可以使用 metadata 来丰富 chunk 的信息，比如 filename、author、category 或者 timestamp，然后就可以基于元数据来做过滤从而限制检索范围。比如检索过程可以为 document 的 timestamp 分配不同的权重从而实现 time-aware 的 RAG，从而确保知识的新鲜性。

还可以人工构建元数据，比如添加段落的 summary，或者引入假设性 question（被称为反向 HyDE）。

#### 3）结构索引

增强信息检索的一种有效方法就是为 document 建立层次结构，比如分层索引结构、知识图谱索引结构等。

### 2.4 查询优化（Query Optimization）

Naive RAG 的一个挑战是：它直接依赖于 user 的原始查询作为检索的基础，原始查询可能由于本身表达存在问题、与知识库存在 gap 等原因，导致检索效果不好。另外，LLM 在处理具有多种含义的专业词汇缩写时也会遇到困难。

所以，对 user query 进行优化，使之变为一个效果更好的查询是一个重要的改进方向。

#### 1）Query Expansion

思路是为一个 single query 丰富其内容，提供更多的上下文，从而确保生成的答案的最佳相关性。具体实现方法包括：

- **Multi-Query**：比如 *RAG-Fusion*，将一个 user query 扩展成多个 query 并行执行。
- **Sub-Query**：将一个 query 规划为多个 sub-query，这些 sub-query 的组合可以更加丰富地回答 original question。比如，**least-to-most prompting* 方法将一个 complex question 分解为多个 simpler sub-questions 来解决。
- **Chain-of-Verification**：扩展后的 query 经过 LLM 验证可以表现出更好的可靠性。可以参考 *Chain-of-Verification* 论文。

#### 2）Query Transformation

核心思想是基于一个 transformed query 而不是 user original query 去检索 chunks。具体实现方法包括：

- **Query Rewrite**：原始的查询在现实世界中并不一定是一个最好的检索，因此可以通过 prompt LLM 的方式去 rewrite queries。相关工作可以参考 EMNLP 2023 的 *Query Rewriting in Retrieval-Augmented Large Language Models*。
- Query 
