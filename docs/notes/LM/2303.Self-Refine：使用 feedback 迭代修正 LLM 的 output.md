---
title: 🐋 Self-Refine：使用 feedback 迭代修正 LLM 的 output
permalink: /arxiv/2303.17651/
author: Bin Yu
createTime: 2024/06/24 22:35:00
---

> 论文：[Self-Refine: Iterative Refinement with Self-Feedback](https://proceedings.neurips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html)
>
> ⭐⭐⭐⭐
>
> CMU, NeurIPS 2023, arXiv:2303.17651
>
> Code: [https://selfrefine.info/](https://selfrefine.info/)

## 论文速读

本文提出了 <mark>Self-Refine</mark> 的 prompt 策略，可以在无需额外训练的情况下，在下游任务上产生更好的效果。

该方法的直观 insight：我们在写一封 email 时，往往写出一个 draft，然后再修改其中措辞不当的地方，修改为更好的版本。

其思路如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240625100205.png" alt="20240625100205" style="zoom:75%;"></center>

- 首先，给定一个 input $x$，在 prompt $p_{gen}$ 下让 LLM 先生成一个初始 output $y_0$
- 进行迭代，每一轮 $t$ 中：
  - **Feedback**：将 input $x$、上一轮 output $y_t$ 和 prompt $p_{fb}$ 给 LLM，得到这一轮的 feedback $fb_t$
  - **Refine**：将 input $x$、历史的所有 feedback 和 output、prompt $p_{refine}$ 给 LLM，得到这一轮的 output $t_{t+1}$

如此迭代，直到 feedback 中被检查出有 stop 标识符，或者达到了最大迭代次数。

下面是一个使用 Self-Refine 来进行 code optimization 的示例：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240625110857.png" alt="20240625110857" style="zoom:75%;"></center>

## 总结

论文提出了 Self-Refine，核心就是反复迭代 Feedback 和 Refine 操作，从而让 LLM 在具体任务上有更好的表现。

论文在多个任务上进行了实验，发现 Self-Refine 可以有效地在各种任务上提升 LLM 的表现，当在较弱的小模型上则表现不佳（会重复输出）。
