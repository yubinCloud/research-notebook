---
title: 🌙 QDecomp：探索 CoT-style 的 prompt 来解决 Text2SQL
author: Bin Yu
createTime: 2024/06/04 16:59:00
permalink: /arxiv/2305.14215/
---

> 论文：[Exploring Chain of Thought Style Prompting for Text-to-SQL](https://aclanthology.org/2023.emnlp-main.327)
>
> ⭐⭐⭐⭐
>
> EMNLP 2023, arXiv:2305.14215

## 一、论文速读

本文通过对 LLM 使用 CoT-style 的 prompting 方法来解决 Text2SQL 问题，试图回答下面两个问题：

1. 哪种 prompting style 更好：在一个 pass 中生成所有推理步骤好，还是迭代 prompting 并解决问题好？
2. 详细的推理步骤对于 Text2SQL 任务来说是否会产生更好的结果？

论文在四种多步推理的 prompting 方法上做了试验，并对比了效果，下面分别介绍。

## 二、用于 Text2SQL 的多步推理的 prompting 方法

下图是四种 prompting 的示例，输入的是相同的 DB schema 和 question，不同的 prompting 方法有期待的不同的输出：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717504155608.png" alt="1717504155608" style="zoom:75%;"></center>

### 2.1 Chain-of-Thought Prompting

CoT 旨在在预测答案之前先生成一系列中间步骤从而提高 LLM 的推理能力，那如何提出这些中间步骤就是一个问题。

本文是使用 SQL query 中的每个 clause 的推理思路组成 CoT prompt 中的推理步骤。下图是一个在 Spider 上 1-shot 的 CoT 示例：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717505610202.png" alt="1717505610202" style="zoom:75%;"></center>

- 红色方框的部分是 1-shot 的 demonstration，教会 LLM 来生成思维链的推理步骤
- 蓝色方框的部分是我们想让 LLM 解决的 question

### 2.2 Least-to-Most Prompting

CoT 是让 LLM 一次生成所有的推理步骤，而 Least-to-Most Prompting 则是在两个阶段来解决复杂问题：

- **problem reduction 阶段**：提示 LLM 从原始复杂 question 中生成一系列 sub-question
- **problem solving 阶段**：每次用一个 sub-question 来 prompt LLM，并迭代地建立最终解决方案

> 最后一个 sub-question 就是原来的 user question。

下图是 problem reduction 阶段的示例：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717506774090.png" alt="1717506774090" style="zoom:75%;"></center>

下图是 problem solving 阶段的示例：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717507167824.png" alt="1717507167824" style="zoom:75%;"></center>

### 2.3 Question Decomposition Prompting（QDecomp）

这里就是本文新提出的 <mark>QDecomp</mark> prompting 方法，这个方法结合 CoT 和 Least-to-Most 两种方法的特点：像 CoT 那样一次性生成中间推理步骤和最终的 SQL query，但是中间推理步骤不是之前使用 CoT 那样的 SQL 逻辑推理过程，而是遵循 Least-to-Most prompting 的 problem reduction 阶段的思路，指导 LLM 将原始复杂问题分解为推理步骤：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717507972711.png" alt="1717507972711" style="zoom:75%;"></center>

在 QDecomp 基础上，本文又提出了一个变体 <mark>QDecomp + InterCOL</mark>，用于缓解 Text2SQL 任务中常见的 Schema Linking 的问题：他扩充了更多的 in-context exemplars，以教会 LLM 在生成每个 sub-questions 时识别出任何相应的 table/column names。

下图是一个示例：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1717508828195.png" alt="1717508828195" style="zoom:75%;"></center>

> 这里的具体解释可以参考原论文

## 三、实验

论文使用了 Spider 和 Spider Realistic 两个数据集，ICL 的 exemplars 是采用 random selection 得到的。LLM 使用 Codex。Prompt Format 使用了 API Docs 的格式。

具体实验上，测试了不同的 prompting 方法，顺带还有 standard prompting 方法作为 baseline。

> standard prompting 方法指的是使用几个 question-SQL pairs 作为 ICL 的 exemplars 来直接 prompt LLM 得到答案，整个过程不涉及中间推理过程。

实验数据如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240604215320.png" alt="20240604215320" style="zoom:75%;"></center>

得出如下结论：

- QDecomp 和 QDecomp + InterCOL 方法的表现均超过了 CoT 和 Least-to-Most
- 对于 Text2SQL 来说，迭代式地解决一系列 sub-questions 可能不是必需的，况且这种方法的成本还很高。
- CoT prompting 甚至比 standard prompting 效果更差

### 3.1 误差分析：CoT 效果甚至不如 standard prompting

通过对 error cases 的分析，发现，**由于 CoT 会生成详细的推理步骤，而这中间任何一步出错都会导致错误传播，从而导致最后的答案出现错误**。

而 QDecomp 方法不指示 LLM 生成详细的推理步骤或者中间 SQL query，这样就减少了推理步骤中错误累积的可能性。

### 3.2 Prompt 的设计

- **ICL Examples 的选择**：发现 QDecomp + InterCOL 方法具备不错的鲁棒性
- **ICL Examples 的数量**：本文发现当样本数量超过 8 时，带来的增益较小，因此本文使用了 8 个上下文示例
- **ICL Examples 的格式**：测试了两种 prompt 格式：*API Docs* 和 *Create Table + Select 3*

## 四、总结

本文基于 Codex 的 LLM 来探索了 CoT-style 的 prompting 效果，增强了 LLM 对 Text2SQL 解析的推理能力。

论文发现了 CoT 在 Text2SQL 任务中会出现错误传播问题，本文提出的 QDecomp 方法是缓解 LLM 多步骤推理中错误传播问题的尝试之一，之后值得对这个问题进行更深一步的探讨。