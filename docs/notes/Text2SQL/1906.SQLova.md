---
title: 🐋 SQLova：首次将 PLM 应用到 NL2SQL 中
author: Bin Yu
createTime: 2024/05/23 20:19:00
permalink: /arxiv/1902.01069/
---

> 论文：[A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization](https://arxiv.org/abs/1902.01069)
>
> ⭐⭐⭐⭐
>
> KR2ML Workshop at NeurIPS 2019, arXiv:1902.01069
>
> Code: [SQLova | GitHub](https://github.com/naver/sqlova)
>
> 参考文章：[将预训练语言模型引入WikiSQL任务 | CSDN](https://blog.csdn.net/u011426236/article/details/135117705)

## 一、论文速度

这篇论文对 SQLNet 进行改进，首次尝试引入 PLM 来获得 context embedding。在实现思路上与 SQLNet 类似，也是先预先构建一个 SQL sketch，然后再填充 slots。

本文提出的模型 <mark>SQLova</mark> 分为两个 layers：encoding layer 和 NL2SQL layer：

- encoding layer：使用 BERT 来获得 table-aware 和 context-aware 的 question word representation
- NL2SQL layer：使用上一层获得的 encoded representation 来生成 SQL query

在 NL2SQL layer 中，思路与 SQLNet 类似，使用了多个 model 来填充 SQL sketch 中的不同 slots 从而生成 SQL。

## 二、SQLova

分别介绍 SQLova 的两个 layers。

### 2.1 encoding layer

WikiSQL dataset 的输入是 question 和 table headers，输出是生成的 SQL query 和相应的执行结果，如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240524102543.png" alt="20240524102543" style="zoom:75%;"></center>

我们需要将 question 和 table headers 使用 `[SEP]` 分隔符连接起来，开头再加一个 `[CLS]`，然后输入给 BERT，BERT 的最后两层输出被拼接起来作为 encoded representation。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240524103014.png" alt="20240524103014" style="zoom:75%;"></center>

这样，**通过 encoding layer，我们借助 BERT 对 question 和 table headers 进行编码，得到了一个 table-aware representation**。

### 2.2 NL2SQL layer

对于上一层得到的 table-aware representation，**还需要使用 LSTM 做进一步的上下文编码**，这里使用的是 100 维的两层 BiLSTM：一个 question encoder *LSTM-q*，一个 header encoder *LSTM-h*

- $E_n$ 表示 question 的第 n 个 token 经过 LSTM-q 后的 encoding vector
- $D_c$ 表示 header *c* 的经过 LSTM-h 的 encoding vector

之后，我们就使用这两个 encoding 来交给不同的 model 预测 slots，如下图所示（从上往下看，BERT 的输出经过 LSTM 再去预测 slots）：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240524104715.png" alt="20240524104715" style="zoom:75%;"></center>

NL2SQL layer 的 6 个 sub-module 不共享参数。这里还用到了 SQLNet 提出的一个关键技术 Column Attention 机制。

### 2.3 Execution-guided decoding

SQLOVA 使用了 Execution-guided decoding 技术，以减少不可执行的 query 语句。

所谓 <mark>Execution-guided decoding</mark> 就是在输出返回结果时对检查生成的 SQL 序列是否是一条语法正确的 SQL 语句，使模型最终输出的 SQL 语句一定是可以无语法错误执行的。它是通过将候选列表中的 SQL 查询按顺序提供给执行器来执行的，并丢弃那些执行失败或返回空结果的查询。该技术可以参考论文 *Robust Text-to-SQL Generation with Execution-Guided Decoding*.

## 三、实验

文章还给出了 human 的 performance，从结果来看，SQLova 已超过了人类表现。

同时，文章还进行了充分的消融实验以检验各个模块的有效性。可以看出，预训练模型 BERT 的引入对结果有很大提升。即使用词语的上下文对 logical form 的 acc 有很大贡献。
