---
title: 🐋 经典论文：SQLNet
author: Bin Yu
createTime: 2024/05/22 22:36:00
permalink: /arxiv/1711.04436/
---

> 论文：[SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning](https://arxiv.org/abs/1711.04436)
>
> ⭐⭐⭐⭐⭐
>
> ICLR 2018, arXiv:1711.00436
>
> Code: 
> - [SQLNet | paperwithcode](https://paperswithcode.com/paper/sqlnet-generating-structured-queries-from)
> - [SQLNet| GitHub](https://github.com/xiaojunxu/SQLNet)

## 一、论文速读

这篇论文强调了一个问题：<mark>order-matters problem</mark> —— 意思是说，对于一个 SQL query，他有需要相同的等价表述形式，比如说 WHERE 子句中的条件互相交换是等价的，但是对于训练一个 seq2seq 的 model 来说，**选择其中的一个是敏感的**。

之前的 Seq2SQL 模型中，是使用强化学习来解决的 order-matter 问题，根据生成的 SQL query 是否正确来计算奖励并用于训练模型。但后来的工作发现通过强化学习来实现的改进是很有限的。

这篇论文提出了 <mark>SQLNet 模型</mark>，它采用基于 sketch（草图）的方法来生成 SQL。**具体来说**，论文预先设计了如下左图所示的一个 **SQL sketch**，其中 `$xxx` 是 **slots**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523104410.png" alt="20240523104410" style="zoom:75%;"></center>

然后使用多个模型来填充这些 slots，上图的右图中展示了 slots 之间的依赖关系，比如 SELECT AGGREGATOR 依赖于 SELECT Column 和 question，表示在预测 AGGREGATOR 时，需要将已经预测到 SELECT Column 和 question 作为条件来预测 AGGREGATOR。

从上图的依赖关系可以看出，不同的 WHERE 子句的预测是互相独立的，由此可以避免 order-matter problem。

## 二、SQLNet 的一些细节

### 2.1 Sequence-to-set

论文使用 seq2set 来将一个 question 转为一个包含 column names 的集合，这个集合的列名就是 WHERE 语句的列名，由此来预测 WHERE 语句包含哪些条件。

具体来说，就是计算 $P_{wherecol}(col|Q)$ 这个概率值，其中 col 是列名，Q 是 NL question，具体计算公式可以参考原论文。

> 原论文在推导这个概率的计算公式是，还提出了一个 Column attention 的概念。

### 2.2 预测 WHERE 子句

WHERE 子句是 WikiSQL 任务中最复杂的预测结构，它的预测也是最有挑战的任务。

在计算出 $P_{wherecol}(col|Q)$ 概率后，选择哪些 columns 作为 WHERE 子句的列呢？论文给出两种方法：

- 设置一个 threshold，概率大于这个阈值的 column 被选中
- 由另一个模型根据 question 生成一个数字 K，并选中 top-K 的 columns

### 2.3 预测 SELECT 子句

预测 SELECT 子句与 WHERE 子句挺相似的，只需要在所有 columns 种选出一个 column 即可，aggregator 的预测也是一个分类器。

### 2.4 训练 loss

给定一个 question Q，一个包含 C 列的 *col*，假设 *y* 是一个 C 维的 vector，其中 $y_i = 1$ 表示第 i 个 column 是真的出现在了正确 SQL 的 WHERE 子句中，$y_i = 0$ 表示没有。使用如下加权负对数 loss 函数来训练 $P_{wherecol}$ 子模型：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523112641.png" alt="20240523112641" style="zoom:75%;"></center>

其中权重 $\alpha$ 是一个超参数，用来平衡 positive data 和 negative data。

### 2.5 权重共享的细节

论文发现，预测不同 slots 所使用的 LSTM 不共享权重更好，同时共享 word embedding 更好。

word embedding 模型采用了 GloVe 模型。

## 三、Column Attention ⭐

这篇论文的一个著名贡献是提出了 <mark>Column Attention</mark>，这一节对其进行介绍。

在前面介绍 seq2set 中，$P_{wherecol}(col|Q)$ 的计算公式如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523215042.png" alt="20240523215042" style="zoom:75%;"></center>

其中：

- *col* 是 table schema 的一个 column name
- *Q* 是自然语言问句
- $E_{col}$ 和 $E_{Q}$ 是 column name 和 question 的 d 维 embedding
- $u_c$ 和 $u_q$ 是可训练的 d 维 vector
- $\sigma$ 是一个 sigmoid function，用来将一个实数转为一个 0~1 的概率值

这个公式的思路是，利用 col name 和 question 的 embedding 联合起来计算概率 $P$ 来决定这个 column 是否被选中。

但是这里有一个问题：$E_Q$ 只是 question 的 embedding，在使用时无法记住用于预测某个 col name 的特定信息，所以一个直觉是，在预测某一 column 时，embedding 应该反映出自然语言问题中最相关的信息。

因此，论文想计算出一个 $E_{Q|col}$ 来代替 $E_Q$，这一节要介绍的 Column Attention 就是用来计算 $E_{Q|col}$ 的。

这里假设 $H_Q$ 是一个 $d \times L$ 的矩阵，L 是 question 的长度，第 i 列 vector 对应的是 question 的第 i 个 token 的 embedding。

之后，我们计算 question 中每个 token 的 attention weights $w$，它也是一个 L 维 vector，其中 $w_i$ 是第 i 个 token 的 weight。其计算方式是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523220744.png" alt="20240523220744" style="zoom:75%;"></center>

其中 $v_i$ 是 v 的第 i 个维度，也是一个标量；$H_Q^i$ 表示 $H_Q$ 的第 i 个列，是一个 L 维的 vector，是第 i 个 token 的 embedding；$W$ 是一个可训练的 $d \times d$ 矩阵，代表着一个仿射变换。

由此，便联合了 column name 和 question 的 embedding 计算出 attention weights $w$，这样，$E_{Q|col}$ 就可以基于 attention weights 和 question embedding $H_Q$ 来计算出来：

$$E_{Q|col} = H_Q \dot w$$

由此计算出来的 $E_{Q|col}$ 就可以用来替换 $E_Q$ 来计算 $P_{wherecol}$。

总结下来，令 $E_{col}$ 是 col name 的 embedding，$E_n$ 是 question 的第 n 个 token 的 embedding，那 Column Attention 的关键就是计算：$v_i = (E_{col})^T W (E_n)$ 从而能够联合 column 和 question。
