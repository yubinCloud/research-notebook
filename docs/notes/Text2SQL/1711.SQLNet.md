---
title: 🐋 经典论文：SQLNet
author: Bin Yu
createTime: 2024/05/22 22:36:00
permalink: /arxiv/1711.04436/
---

> 论文：[SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning](https://arxiv.org/abs/1711.04436)
>
> ⭐⭐⭐⭐
>
> Code: 
> - [SQLNet | paperwithcode](https://paperswithcode.com/paper/sqlnet-generating-structured-queries-from)
> - [SQLNet| GitHub](https://github.com/xiaojunxu/SQLNet)

## 一、论文速读

这篇论文强调了一个问题：<mark>order-matters problem</mark> —— 意思是说，对于一个 SQL query，他有需要相同的等价表述形式，比如说 WHERE 子句中的条件互相交换是等价的，但是对于训练一个 seq2seq 的 model 来说，**选择其中的一个是敏感的**。

之前的 Seq2SQL 模型中，是使用强化学习来解决的 order-matter 问题，根据生成的 SQL query 是否正确来计算奖励并用于训练模型。但后来的工作发现通过强化学习来实现的改进是很有限的。

这篇论文提出了 <mark>SQLNet 模型</mark>，它采用基于 sketch（草图）的方法来生成 SQL。**具体来说**，论文预先设计了如下左图所示的一个 **SQL sketch**，其中 `$xxx` 是 **slots**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523104410.png" alt="20240523104410" style="zoom:75%;"></center>

然后使用多个模型来填充这些 slots，上图的右图中展示了 slots 之间的依赖关系，比如 SELECT AGGREGATOR 依赖于 SELECT Column 和 question，表示在预测 AGGREGATOR 时，需要将已经预测到 SELECT Column 和 question 作为条件来预测 AGGREGATOR。

从上图的依赖关系可以看出，不同的 WHERE 子句的预测是互相独立的，由此可以避免 order-matter problem。

## 二、SQLNet 的一些细节

### 2.1 Sequence-to-set

论文使用 seq2set 来将一个 question 转为一个包含 column names 的集合，这个集合的列名就是 WHERE 语句的列名，由此来预测 WHERE 语句包含哪些条件。

具体来说，就是计算 $P_{wherecol}(col|Q)$ 这个概率值，其中 col 是列名，Q 是 NL question，具体计算公式可以参考原论文。

> 原论文在推导这个概率的计算公式是，还提出了一个 Column attention 的概念。

### 2.2 预测 WHERE 子句

WHERE 子句是 WikiSQL 任务中最复杂的预测结构，它的预测也是最有挑战的任务。

在计算出 $P_{wherecol}(col|Q)$ 概率后，选择哪些 columns 作为 WHERE 子句的列呢？论文给出两种方法：

- 设置一个 threshold，概率大于这个阈值的 column 被选中
- 由另一个模型根据 question 生成一个数字 K，并选中 top-K 的 columns

### 2.3 预测 SELECT 子句

预测 SELECT 子句与 WHERE 子句挺相似的，只需要在所有 columns 种选出一个 column 即可，aggregator 的预测也是一个分类器。

### 2.4 训练 loss

给定一个 question Q，一个包含 C 列的 *col*，假设 *y* 是一个 C 维的 vector，其中 $y_i = 1$ 表示第 i 个 column 是真的出现在了正确 SQL 的 WHERE 子句中，$y_i = 0$ 表示没有。使用如下加权负对数 loss 函数来训练 $P_{wherecol}$ 子模型：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20240523112641.png" alt="20240523112641" style="zoom:75%;"></center>

其中权重 $\alpha$ 是一个超参数，用来平衡 positive data 和 negative data。

### 2.5 权重共享的细节

论文发现，预测不同 slots 所使用的 LSTM 不共享权重更好，同时共享 word embedding 更好。

word embedding 模型采用了 GloVe 模型。
