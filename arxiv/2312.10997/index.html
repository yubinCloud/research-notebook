<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.9" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.55" /><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;if (um === 'dark' || (um !== 'light' && sm)) {document.documentElement.classList.add('dark');}})();</script><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><title>🐋 RAG for LLM 综述（23年12月）</title><meta name="description" content=""><link rel="preload" href="/research-notebook/assets/style-Cec0e0LW.css" as="style"><link rel="stylesheet" href="/research-notebook/assets/style-Cec0e0LW.css"><link rel="modulepreload" href="/research-notebook/assets/app-s5JIphqh.js"><link rel="modulepreload" href="/research-notebook/assets/index.html-D8yUAJ7Y.js"><link rel="prefetch" href="/research-notebook/assets/index.html-B-j-ryWq.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-D4spU_9v.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-B0-1BBu1.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-SLLIY8d9.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C65jmr_M.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C-bL2yAS.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C7B5e00U.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-Nt9kYeWR.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CE_bMlSy.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CLhUsc3g.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-DXgEbOI-.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-v1oOswjF.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CBJ8xjXF.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-D4Fq4d_T.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-ByB-z2iB.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-Q9nU2u_y.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-Dtno56yj.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BDK4L6jX.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-Dc5x4woP.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-eGfCPn-9.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-D70LzGEm.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BPemUoR7.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BwgVpDih.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-DlaBYZPe.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CgVNpCXw.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-G5FDBLuS.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-uDKSTe9v.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BVMJJZGD.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CrHd4gIE.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BiSS2sK0.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-DXcBEjAg.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CQoGuxuN.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CRi0y6sX.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BKNM53Nr.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C0kRXJHb.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-QyJX4BVb.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CJvsDrCW.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BwT074si.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-sp3GJ4_Y.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C3rZacVL.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-B0q2VZS4.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CYDgqNcV.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CDChQk7d.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-C9fq1LXw.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-_hma_Fnn.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-7G48gGCA.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BlT-qKyg.js" as="script"><link rel="prefetch" href="/research-notebook/assets/404.html-V2miJEAG.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BcMo9zlU.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-CrhXs5dX.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-DEOpBtft.js" as="script"><link rel="prefetch" href="/research-notebook/assets/index.html-BKeLltu8.js" as="script"><link rel="prefetch" href="/research-notebook/assets/SearchBox-CGeHVkxs.js" as="script"></head><body><div id="app"><!--[--><div class="theme-plume" data-v-ba475dda><!--[--><!--[--><span tabindex="-1" data-v-4bbc90d0></span><a href="#LayoutContent" class="skip-link visually-hidden" data-v-4bbc90d0> Skip to content </a><!--]--><!----><div class="nav-wrapper" data-v-ba475dda data-v-7a402866><div class="navbar-wrapper" data-v-7a402866 data-v-c0f362fa><div class="container" data-v-c0f362fa><div class="title" data-v-c0f362fa><div class="navbar-title" data-v-c0f362fa data-v-b77839f9><a class="auto-link link title" href="/" data-v-b77839f9 data-v-5111f611><!--[--><!----> <!--]--><!----></a></div></div><div class="content" data-v-c0f362fa><div class="curtain" data-v-c0f362fa></div><div class="content-body" data-v-c0f362fa><div class="navbar-search search" data-v-c0f362fa><div class="search-wrapper" data-v-60fa285e><!----><div id="local-search" data-v-60fa285e><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-60fa285e><span class="mini-search-button-container"><svg class="mini-search-search-icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><nav aria-labelledby="main-nav-aria-label" class="navbar-menu menu" data-v-c0f362fa data-v-b0c203ca><span id="main-nav-aria-label" class="visually-hidden" data-v-b0c203ca>Main Navigation</span><!--[--><!--[--><a class="auto-link link navbar-menu-link" href="/category/LM/" data-v-b0c203ca data-v-c7242555 data-v-5111f611><!--[--><!----><i data-v-c7242555>LLM</i><!--]--><!----></a><!--]--><!--[--><a class="auto-link link navbar-menu-link" href="/category/LLM-evaluation/" data-v-b0c203ca data-v-c7242555 data-v-5111f611><!--[--><!----><i data-v-c7242555>LLM 评估</i><!--]--><!----></a><!--]--><!--[--><a class="auto-link link navbar-menu-link" href="/category/RAG/" data-v-b0c203ca data-v-c7242555 data-v-5111f611><!--[--><!----><i data-v-c7242555>RAG</i><!--]--><!----></a><!--]--><!--[--><a class="auto-link link navbar-menu-link" href="/category/Text2SQL/" data-v-b0c203ca data-v-c7242555 data-v-5111f611><!--[--><!----><i data-v-c7242555>Text2SQL</i><!--]--><!----></a><!--]--><!--]--></nav><!----><div class="navbar-appearance appearance" data-v-c0f362fa data-v-376ec936><button class="switch-wrapper switch-appearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-376ec936 data-v-b27b6603 data-v-f90faf21><span class="check" data-v-f90faf21><span class="icon" data-v-f90faf21><!--[--><span class="vpi-sun sun" data-v-b27b6603></span><span class="vpi-moon moon" data-v-b27b6603></span><!--]--></span></span></button></div><!----><div class="flyout-wrapper navbar-extra extra" data-v-c0f362fa data-v-7914c0f6 data-v-7e56bc7a><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-7e56bc7a><span class="vpi-more-horizontal icon" data-v-7e56bc7a></span></button><div class="menu" data-v-7e56bc7a><div class="menu-wrapper" data-v-7e56bc7a data-v-b4fba565><!----><!--[--><!--[--><!----><div class="group" data-v-7914c0f6><div class="item appearance" data-v-7914c0f6><p class="label" data-v-7914c0f6>外观</p><div class="appearance-action" data-v-7914c0f6><button class="switch-wrapper switch-appearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7914c0f6 data-v-b27b6603 data-v-f90faf21><span class="check" data-v-f90faf21><span class="icon" data-v-f90faf21><!--[--><span class="vpi-sun sun" data-v-b27b6603></span><span class="vpi-moon moon" data-v-b27b6603></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><button type="button" class="navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-c0f362fa data-v-7e5802de><span class="container" data-v-7e5802de><span class="top" data-v-7e5802de></span><span class="middle" data-v-7e5802de></span><span class="bottom" data-v-7e5802de></span></span></button></div></div></div></div><!----></div><!----><!----><div id="LayoutContent" class="layout-content" data-v-ba475dda data-v-094016f3><!--[--><div class="has-aside plume-page" data-v-5b0e3d8f><div class="container" data-v-5b0e3d8f><div class="aside" data-v-5b0e3d8f><div class="aside-container" data-v-5b0e3d8f><div class="aside-content" data-v-5b0e3d8f><div class="page-aside" data-v-5b0e3d8f data-v-f8c78049><div class="has-outline page-aside-outline" data-v-f8c78049><div class="content" data-v-f8c78049><div class="outline-marker" data-v-f8c78049></div><div class="outline-title" data-v-f8c78049><span data-v-f8c78049>此页内容</span><span class="vpi-print icon" data-v-f8c78049></span></div><nav aria-labelledby="doc-outline-aria-label" data-v-f8c78049><span id="doc-outline-aria-label" class="visually-hidden" data-v-f8c78049> Table of Contents for current page </span><ul class="root" data-v-f8c78049 data-v-9a83b7a1><!--[--><li data-v-9a83b7a1><a class="outline-link" href="#一、rag-的三种研究范式-naive、advanced、modular" data-v-9a83b7a1>一、RAG  的三种研究范式：Naive、Advanced、Modular</a><ul class="nested" data-v-9a83b7a1 data-v-9a83b7a1><!--[--><li data-v-9a83b7a1><a class="outline-link" href="#_1-1-naive-rag" data-v-9a83b7a1>1.1 Naive RAG</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_1-2-advanced-rag" data-v-9a83b7a1>1.2 Advanced RAG</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_1-3-modular-rag" data-v-9a83b7a1>1.3 Modular RAG</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_1-4-rag-vs-fine-tuning" data-v-9a83b7a1>1.4 RAG vs Fine-tuning</a><!----></li><!--]--></ul></li><li data-v-9a83b7a1><a class="outline-link" href="#二、retrieval" data-v-9a83b7a1>二、Retrieval</a><ul class="nested" data-v-9a83b7a1 data-v-9a83b7a1><!--[--><li data-v-9a83b7a1><a class="outline-link" href="#_2-1-retrieval-的数据结构" data-v-9a83b7a1>2.1 Retrieval 的数据结构</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_2-2-retrieval-粒度" data-v-9a83b7a1>2.2 Retrieval 粒度</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_2-3-索引优化-indexing-optimization" data-v-9a83b7a1>2.3 索引优化（Indexing Optimization）</a><!----></li><li data-v-9a83b7a1><a class="outline-link" href="#_2-4-查询优化-query-optimization" data-v-9a83b7a1>2.4 查询优化（Query Optimization）</a><!----></li><!--]--></ul></li><!--]--></ul></nav></div></div></div></div></div></div><div class="content" data-v-5b0e3d8f><div class="content-container" data-v-5b0e3d8f><!--[--><!----><h1 class="page-title" data-v-dbc950ba>🐋 RAG for LLM 综述（23年12月）</h1><div class="page-meta-wrapper" data-v-dbc950ba><p class="author" data-v-dbc950ba><span class="icon vpi-user" data-v-dbc950ba></span><span data-v-dbc950ba>Bin Yu</span></p><p class="reading-time" data-v-dbc950ba><span class="vpi-books icon" data-v-dbc950ba></span><span data-v-dbc950ba>2847字</span><span data-v-dbc950ba>约9分钟</span></p><!----><p class="create-time" data-v-dbc950ba><span class="vpi-clock icon" data-v-dbc950ba></span><span data-v-dbc950ba>2024-05-01</span></p></div><!--]--><!--[--><div style="position:relative;" data-v-5b0e3d8f><div class="plume-content" data-v-5b0e3d8f><blockquote><p>论文：<a href="http://arxiv.org/abs/2312.10997" target="_blank" rel="noopener noreferrer">Retrieval-Augmented Generation for Large Language Models: A Survey<span class="vpi-external-link external-icon" data-v-9f412e4e></span></a></p><p>⭐⭐⭐⭐</p><p>同济，arXiv:2312.10997</p></blockquote><h2 id="一、rag-的三种研究范式-naive、advanced、modular" tabindex="-1"><a class="header-anchor" href="#一、rag-的三种研究范式-naive、advanced、modular"><span>一、RAG 的三种研究范式：Naive、Advanced、Modular</span></a></h2><p>RAG 的研究范式持续进化，这里可以将其分成三个阶段：Naive RAG、Advanced RAG 和 Modular RAG。三种范式的对比如下图：</p><p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202418233.png" alt="image-20240510202418233"></p><h3 id="_1-1-naive-rag" tabindex="-1"><a class="header-anchor" href="#_1-1-naive-rag"><span>1.1 Naive RAG</span></a></h3><p>Naive RAG 是 <em>Retrieve-Read</em> 框架，包含了 indexing、retrieval 和 generation 三个过程：</p><ul><li>Indexing：对原始的 PDF、HTML、Word 或者 Markdown 等格式的数据进行清洗和抽取，转化为统一的 plain text 格式，并进而将其切分为更小的 chunks，之后，使用 embedding model 将这些 chunks 编码为 vector 并存入 vector db。这一步骤便于之后检索过程时做 similarity 计算。</li><li>Retrieval：对用户的 user query 使用 encoder 模型将其转化为 vector，然后计算 query vector 和 chunks vector 之间的相似度，从 corpus 中找到最相似的 K 个 chunks，这些检索得到的 chunks 被用于扩展之后 prompt 的上下文。</li><li>Generation：将 prompt template + user query + retrieved docs 输入给 LLM，让其完成响应生成。</li></ul><p>但是 Naive RAG 具有几个明显的缺点：</p><ul><li>retrieval 阶段可能会检索到不相关的信息，同时丢失一些重要信息</li><li>检索到的不相关的信息很可能会影响 LLM 的回复生成，降低生成的质量</li><li>当从多个 source 中检索到相似信息时，可能导致 LLM 出现重复响应的问题（即同样意思的话会反复说多次）</li><li>具有复杂推理的问题（如 multi-hop 问题），可能单次检索不足以获取足够的上下文信息</li><li>LLM 可能会过度依赖检索到的信息，导致 LLM 的输出只反映了检索到的内容，却没有添加深入思考或者合成的信息</li></ul><h3 id="_1-2-advanced-rag" tabindex="-1"><a class="header-anchor" href="#_1-2-advanced-rag"><span>1.2 Advanced RAG</span></a></h3><blockquote><p>可参考资料：<a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6" target="_blank" rel="noopener noreferrer">Advanced RAG Techniques: an Illustrated Overview<span class="vpi-external-link external-icon" data-v-9f412e4e></span></a></p></blockquote><p>Advanced RAG 以提高检索质量为重点来改进 Naive RAG。在 Naive RAG 的基础上，它引入了 pre-retrieval 和 post-retrieval 的阶段，在这两个阶段里，可以施加一些改进检索的技术。</p><ul><li><p><strong>Pre-retrieval process</strong>：在做检索之前加的一个阶段，主要关注的是<strong>优化索引和优化原始查询</strong>：</p><ul><li><p>优化索引的目标是提高被索引内容的质量。可以使用的策略包括：增强数据粒度、优化索引结构、添加元数据、对齐优化、混合检索等等</p></li><li><p>优化原始查询的目标是让 user query 的问题更加清晰，使之更加适合检索任务。可以使用的策略有：查询重写、查询转换、查询扩展等</p></li></ul></li><li><p><strong>Post-retrieval process</strong>：在检索到相关文档后，直接将所有相关文档输入给 LLM 会让它信息过载，不相关的信息会淡化 LLM 对关键细节的理解，甚至会对其产生误导。因此，这一阶段主要可以做的工作包括 re-rank 和 context compressing:</p><ul><li><p>re-rank：对检索到的文档重新排序，将最相关的内容放到前面</p></li><li><p>context compressing：压缩检索到的上下文，尽量只保留关键信息</p></li></ul></li></ul><h3 id="_1-3-modular-rag" tabindex="-1"><a class="header-anchor" href="#_1-3-modular-rag"><span>1.3 Modular RAG</span></a></h3><p>Modular RAG 通过引入多种策略、增加多个模块来提升 RAG 的各个相关组件。</p><ul><li>它允许根据具体的问题来调整模块和流程，例如引入搜索模块、记忆模块、额外生成模块等新模块，以扩展 RAG 的功能。</li><li>允许调整模块之间的顺序和连接方式，如对齐模块、添加或替换模块等，以适应不同任务和场景，与 LLM 的其他技术（如提示工程、知识蒸馏等）相结合，提高模型性能，并最终通过调整模块和流程，使 RAG 技术能够适应各种下游任务，提高通用性。</li></ul><p><strong>New Modules</strong> 的相关工作有：</p><ul><li>KnowledGPT：将检索数据源扩展到了 Knowledge Base</li><li>RAF-Fusion：引入 multi-query 策略，来扩展 user query 的视角，从而覆盖更加广阔的检索面</li><li>Selfmem：引入 memory module，memory 包含检索到的文档和 LLM 自己之前的生成，并使用 memory 来增强 generator</li><li>GenRead：将 retrieve-then-read 改为 generate-then-read 来提升效果，并提出进一步两者融合的思路</li><li>UPRISE：使用 prompt retriever 来检索 prompt 从而增强 LLM 的 zero-shot 能力</li></ul><p><strong>New Patterns</strong> 的相关工作有：</p><ul><li>Rewrite-Retrieve-Read：利用 LLM 的能力去修正 user query</li><li>GenRead：使用 Generate-Read 替换了 Retrieval-Read</li><li>Recite-Read：通过 prompt 让 LLM 先背诵再回答，从而将原先的知识密集型任务分解为两个步骤：knowledge-recitation 和 task-execution</li><li>HyDE：让 LLM 先根据 user query 生成一个杜撰的 doc，再使用杜撰的 doc 来通过 embedding 相似度来检索相关的 doc</li><li>Self-RAG：将 retrieval 改进为“自我反思检索”，让 LLM 每次生成一个文本段，生成前先反思一下是否需要检索，并利用相关机制从多个检索候选文档中选出最合适的文档</li></ul><h3 id="_1-4-rag-vs-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_1-4-rag-vs-fine-tuning"><span>1.4 RAG vs Fine-tuning</span></a></h3><p>用一个四象限图来表示如下，从 external knowledge 和 model adaption required 两个维度来阐释：</p><p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20240510202714708.png" alt="image-20240510202714708"></p><p>对于 RAG 和 FT 的选择依赖于具体的场景。其实，RAG 和 FT 并没互相排斥的技术，而是而可以相互补充并在不同的层次上来提高 model 的能力。</p><h2 id="二、retrieval" tabindex="-1"><a class="header-anchor" href="#二、retrieval"><span>二、Retrieval</span></a></h2><p>RAG 依赖 retrieval 来从 data source 中获取相关文档。retrieval 需要考虑的问题有：检索源、检索粒度、检索的预处理、嵌入模型的选择等等。</p><h3 id="_2-1-retrieval-的数据结构" tabindex="-1"><a class="header-anchor" href="#_2-1-retrieval-的数据结构"><span>2.1 Retrieval 的数据结构</span></a></h3><ul><li><mark><strong>Unstructured Data</strong></mark>：比如文本。文本是使用最广泛的检索源，包括 Wikipedia Dump、domain-specific 数据。开放域 QA 主要使用的是 Wikipedia Dump，包括 HotpotQA、DPR 等。</li><li><mark><strong>Semi-structured Data</strong></mark>：通常指包含文本和表格信息的组合的数据，例如 PDF。处理半结构化数据对于 RAG 来说是很有挑战的，因为在融合文本和表结构存在很多困难，可以参考相关领域研究。</li><li><mark><strong>Structured Data</strong></mark>：比如 Knowledge Graph，这些信息经过了验证，可以提供更加精确的信息，比如 KnowledGPT 就是利用 KB 来增强 RAG 模型。当然另一方面，这种结构化数据需要额外的努力去构建、验证和维护结构化的数据库。</li><li><mark><strong>LLMs-Generated Content</strong></mark>：RAG 强调引入外部辅助知识，但其实利用好 LLM 内部的参数化知识也很重要。像 SKR 论文就将问题划分为已知和未知，已知问题直接交由 LLM 依靠内部知识来解决。像 Selfmem 论文就是也将 LLM 生成的信息作为增强上下文。</li></ul><h3 id="_2-2-retrieval-粒度" tabindex="-1"><a class="header-anchor" href="#_2-2-retrieval-粒度"><span>2.2 Retrieval 粒度</span></a></h3><p>检索粒度的选择会明显影响模型的效果，因为 LLM 会受到无关上下文的影响并因此导致生成质量降低。</p><ul><li>粗粒度的检索单元可以为 question 提供更多相关信息，但也会包含较多的冗余内容从而分散 retriever 和 LLM 在下游任务的注意力。</li><li>细粒度的检索单元让相关信息的密度更大，但会增加检索的负担，并且不能保证语义的完整性和所需知识的满足。</li></ul><p>在文本数据中，检索粒度从细到粗可以大致分为：token、phrase、sentence、proposition、chunk、document。</p><p>在 Knowledge Graph 中，检索粒度包括：entity、triplet、sub-graph。</p><blockquote><p>proposition 的检索粒度可以参考 Dense X 工作。</p></blockquote><h3 id="_2-3-索引优化-indexing-optimization" tabindex="-1"><a class="header-anchor" href="#_2-3-索引优化-indexing-optimization"><span>2.3 索引优化（Indexing Optimization）</span></a></h3><p>indexing 阶段的工作包括：将 documents 预处理、分割，并转换为 embedding 存入 vector database 中。</p><p>索引构建的质量决定了能否在检索阶段获得正确的上下文。</p><h4 id="_1-chunk-策略" tabindex="-1"><a class="header-anchor" href="#_1-chunk-策略"><span>1）chunk 策略</span></a></h4><p>最常见的方法就是按照固定数量的 tokens 来将 document 切分为 chunks，token 数量一般为 100、256、512。当然，这个过程存在很多优化，需要在语义完整性和无关信息冗余之间做出平衡：</p><ul><li>较大的 chunk 会包含较多的无关信息，影响 LLM 的生成质量，同时难以检索（因为无关信息会影响嵌入表示）</li><li>较小的 chunk 具有更高密度的相关信息，但是语义完整性较差，难以解决多条推理等复杂问题</li></ul><h4 id="_2-元数据附件" tabindex="-1"><a class="header-anchor" href="#_2-元数据附件"><span>2）元数据附件</span></a></h4><p>可以使用 metadata 来丰富 chunk 的信息，比如 filename、author、category 或者 timestamp，然后就可以基于元数据来做过滤从而限制检索范围。比如检索过程可以为 document 的 timestamp 分配不同的权重从而实现 time-aware 的 RAG，从而确保知识的新鲜性。</p><p>还可以人工构建元数据，比如添加段落的 summary，或者引入假设性 question（被称为反向 HyDE）。</p><h4 id="_3-结构索引" tabindex="-1"><a class="header-anchor" href="#_3-结构索引"><span>3）结构索引</span></a></h4><p>增强信息检索的一种有效方法就是为 document 建立层次结构，比如分层索引结构、知识图谱索引结构等。</p><h3 id="_2-4-查询优化-query-optimization" tabindex="-1"><a class="header-anchor" href="#_2-4-查询优化-query-optimization"><span>2.4 查询优化（Query Optimization）</span></a></h3><p>Naive RAG 的一个挑战是：它直接依赖于 user 的原始查询作为检索的基础，原始查询可能由于本身表达存在问题、与知识库存在 gap 等原因，导致检索效果不好。另外，LLM 在处理具有多种含义的专业词汇缩写时也会遇到困难。</p><p>所以，对 user query 进行优化，使之变为一个效果更好的查询是一个重要的改进方向。</p><h4 id="_1-query-expansion" tabindex="-1"><a class="header-anchor" href="#_1-query-expansion"><span>1）Query Expansion</span></a></h4><p>思路是为一个 single query 丰富其内容，提供更多的上下文，从而确保生成的答案的最佳相关性。具体实现方法包括：</p><ul><li><strong>Multi-Query</strong>：比如 <em>RAG-Fusion</em>，将一个 user query 扩展成多个 query 并行执行。</li><li><strong>Sub-Query</strong>：将一个 query 规划为多个 sub-query，这些 sub-query 的组合可以更加丰富地回答 original question。比如，*<em>least-to-most prompting</em> 方法将一个 complex question 分解为多个 simpler sub-questions 来解决。</li><li><strong>Chain-of-Verification</strong>：扩展后的 query 经过 LLM 验证可以表现出更好的可靠性。可以参考 <em>Chain-of-Verification</em> 论文。</li></ul><h4 id="_2-query-transformation" tabindex="-1"><a class="header-anchor" href="#_2-query-transformation"><span>2）Query Transformation</span></a></h4><p>核心思想是基于一个 transformed query 而不是 user original query 去检索 chunks。具体实现方法包括：</p><ul><li><strong>Query Rewrite</strong>：原始的查询在现实世界中并不一定是一个最好的检索，因此可以通过 prompt LLM 的方式去 rewrite queries。相关工作可以参考 EMNLP 2023 的 <em>Query Rewriting in Retrieval-Augmented Large Language Models</em>。</li><li>基于 prompt 工程让 LLM 生成基于原始 query 来生成一个 query：比如 <em>HyDE</em> 让 LLM 先生成一个杜撰文档用于 retrieval；<em>Step-Back Prompting</em> 先针对 user query 生成一个更高层次的 query 来检索相关事实用于辅助回答。</li></ul><h4 id="_3-query-routing" tabindex="-1"><a class="header-anchor" href="#_3-query-routing"><span>3）Query Routing</span></a></h4><p>思想是根据不同 query 路由到某个特定的合适的 RAG pipeline 中，从而可以构建出能够适用于不同场景的通用 RAG 系统。</p><ul><li><strong>Metadata Router / Filter</strong>：先从 query 中提取出关键字或实体，然后根据 chunk 中的关键字或 metadata 来过滤，从而缩小搜索范围</li><li><strong>Semantic Router</strong>：利用 query 的语义信息来路由。详细使用可以参考 <a href="https://github.com/aurelio-labs/semantic-router" target="_blank" rel="noopener noreferrer">github.com/aurelio-labs/semantic-router<span class="vpi-external-link external-icon" data-v-9f412e4e></span></a></li></ul><p>也可以将 metadata 方法与 semantic route 方法结合，来增强 query routing。</p></div><!----></div><footer class="page-footer" data-v-5b0e3d8f data-v-bb1af879><!----><div class="contributors" data-v-bb1af879><span class="contributors-label" data-v-bb1af879>贡献者: </span><span class="contributors-info" data-v-bb1af879><!--[--><!--[--><span class="contributor" title="email: yubin_SkyWalker@yeah.net" data-v-bb1af879>yubinCloud</span><!----><!--]--><!--]--></span></div><!----></footer><!----><!--]--></div></div></div></div><button style="display:none;" type="button" class="back-to-top-button" aria-label="back to top" data-v-ba475dda data-v-b42999a2><span class="percent" data-v-b42999a2>0%</span><span class="show icon vpi-back-to-top" data-v-b42999a2></span><svg aria-hidden="true" data-v-b42999a2><circle cx="50%" cy="50%" style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-b42999a2></circle></svg></button><footer class="plume-footer" data-v-ba475dda data-v-46d530a6><div class="container" data-v-46d530a6><p class="message" data-v-46d530a6>Power by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://github.com/pengzhanbo/vuepress-theme-plume">vuepress-theme-plume</a></p><!----></div></footer><!--]--></div><!--]--></div><!--[--><!--]--><!--]--></div><script type="module" src="/research-notebook/assets/app-s5JIphqh.js" defer></script></body></html>